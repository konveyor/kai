{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent to take follow up actions after analysis issue fix\n",
    "\n",
    "This is an agent that uses \"Additional Information\" from fix generation agent to make further changes.\n",
    "\n",
    "For faster prototyping, we use LangGraph for implementing this.\n",
    "\n",
    "We create a simple network of agents - FixGenerationAgent, FileOperationsAgent. \n",
    "\n",
    "The _FixGenerationAgent_ is the same as what we use today to fix analyzer issues. _FileOperationsAgent_ is a new agent we introduce that takes follow up actions. The follow up actions include updating another file, removing a file, or creating a new file. For each operation, we have a tool defined.\n",
    "\n",
    "![Diagram](./screenshots/Tool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Generation Agent\n",
    "\n",
    "This is same as the existing agent we use to fix analysis issues.\n",
    "\n",
    "We are using a few hardcoded examples of analysis issues to generate fixes.\n",
    "\n",
    "Before proceeding, make sure you are using Kai venv to run cells in this notebook.\n",
    "\n",
    "We need to install `langgraph` module, run following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL CONTAINS SOME COMMON FUNCTIONS WE WILL USE, RUN THIS CELL BEFORE MOVING FORWARD\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_llm_response(message: str) -> tuple[str, str, str]:\n",
    "    lines_of_output = message.splitlines()\n",
    "    in_source_file = False\n",
    "    in_reasoning = False\n",
    "    in_additional_details = False\n",
    "    source_file = \"\"\n",
    "    reasoning = \"\"\n",
    "    additional_details = \"\"\n",
    "    for line in lines_of_output:\n",
    "        # trunk-ignore(cspell/error)\n",
    "        if re.match(r\"(?:##|\\*\\*)\\s+[Rr]easoning\", line.strip()):\n",
    "            in_reasoning = True\n",
    "            in_source_file = False\n",
    "            in_additional_details = False\n",
    "            continue\n",
    "        # trunk-ignore(cspell/error)\n",
    "        if re.match(r\"(?:##|\\*\\*)\\s+[Uu]pdated.*[Ff]ile\", line.strip()):\n",
    "            in_source_file = True\n",
    "            in_reasoning = False\n",
    "            in_additional_details = False\n",
    "            continue\n",
    "        # trunk-ignore(cspell/error)\n",
    "        if re.match(r\"(?:##|\\*\\*)\\s+[Aa]dditional\\s+[Ii]nformation\", line.strip()):\n",
    "            in_reasoning = False\n",
    "            in_source_file = False\n",
    "            in_additional_details = True\n",
    "            continue\n",
    "        if in_source_file:\n",
    "            if re.match(r\"```(?:\\w*)\", line):\n",
    "                continue\n",
    "            source_file = \"\\n\".join([source_file, line])\n",
    "        if in_reasoning:\n",
    "            reasoning = \"\\n\".join([reasoning, line])\n",
    "        if in_additional_details:\n",
    "            additional_details = \"\\n\".join([additional_details, line])\n",
    "    return source_file, reasoning, additional_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the analysis fix agent and generate a few example fixes to be used with file operation agents. \n",
    "\n",
    "The example issues are taken from Coolstore application analysis and can be found in [example_prompts](./example_prompts/) directory.\n",
    "\n",
    "Running the following cell will generate analysis fixes for all examples and store them in variables for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# fix generation agent\n",
    "fix_gen_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    name=\"fix_gen_agent\",\n",
    "    prompt=\"You are an experienced Java developer, who specializes in migrating code from Java EE to Quarkus\",\n",
    "    tools=[],\n",
    ")\n",
    "\n",
    "raw_response_ex1 = fix_gen_agent.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=Path(\"example_prompts\", \"inventory_notification_java\", \"jms-to-reactive-quarkus-00050\").read_text()),\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use following additional info for Example-1:\n",
      "\n",
      "\n",
      "- Ensure your `pom.xml` includes the necessary Quarkus SmallRye Reactive Messaging dependencies. For example:\n",
      "  ```xml\n",
      "  <dependency>\n",
      "    <groupId>io.quarkus</groupId>\n",
      "    <artifactId>quarkus-smallrye-reactive-messaging</artifactId>\n",
      "  </dependency>\n",
      "  ```\n",
      "- Configure the messaging channel in `application.properties` to connect to your message broker.\n",
      "- This refactor assumes that somewhere in the application's configuration, the channel with the name `\"orders\"` is set up.\n",
      "- Further migration steps might include migrating other Java EE specific annotations and configurations to their Quarkus equivalents.\n"
     ]
    }
   ],
   "source": [
    "_, _, additional_info_ex1 = parse_llm_response(raw_response_ex1[\"messages\"][-1].content)\n",
    "print(\"We will use following additional info for Example-1:\")\n",
    "print(additional_info_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Operations Agent\n",
    "\n",
    "This is the agent that will take \"Additional Information\" generated by previous agent as input and take actions if there is anything important in additional information. \n",
    "\n",
    "The actions include:\n",
    " * updating an existing file (which may not be same as the original file we fixed)\n",
    " * deleting a file\n",
    " * creating a new file\n",
    "\n",
    "The agent will use following tools to take the above options:\n",
    " * find: Finds relevant files in the project\n",
    " * read: Reads a file in the project\n",
    " * write: Writes content to a file in the project\n",
    " * rm: Removes a file from the project\n",
    "\n",
    "In the following cell, we define these tools and the agent.\n",
    "\n",
    "The [project](./project/) directory is the base of the project and will be used by the agent to perform file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_r97mhoOlCxYtNBlJvj4FzFyH', 'function': {'arguments': '{\"file_name_pattern\":\"**/pom.xml\"}', 'name': 'find'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 496, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-be4a4137-0eca-4faf-80d0-c1df0f511572-0', tool_calls=[{'name': 'find', 'args': {'file_name_pattern': '**/pom.xml'}, 'id': 'call_r97mhoOlCxYtNBlJvj4FzFyH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 496, 'output_tokens': 19, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml', name='find', id='998a1568-9563-4de6-a20a-9ba1d8c2a5c1', tool_call_id='call_r97mhoOlCxYtNBlJvj4FzFyH')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_CMMZUntPQWq19jAWuOUfgK3S', 'function': {'arguments': '{\"file_name\":\"/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml\"}', 'name': 'read'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 541, 'total_tokens': 575, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4d91d4cf-b1f1-410c-8207-1e42e8fc52b0-0', tool_calls=[{'name': 'read', 'args': {'file_name': '/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml'}, 'id': 'call_CMMZUntPQWq19jAWuOUfgK3S', 'type': 'tool_call'}], usage_metadata={'input_tokens': 541, 'output_tokens': 34, 'total_tokens': 575, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='Here are the file contents:\\n```<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project \\n    xmlns=\"http://maven.apache.org/POM/4.0.0\" \\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n    <groupId>com.redhat.coolstore</groupId>\\n    <artifactId>monolith</artifactId>\\n    <version>1.0.0-SNAPSHOT</version>\\n    <packaging>war</packaging>\\n    <name>coolstore-monolith</name>\\n    <properties>\\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\n        <maven.build.timestamp.format>yyyyMMdd\\'T\\'HHmmss</maven.build.timestamp.format>\\n        <project.encoding>UTF-8</project.encoding>\\n        <maven.test.skip>true</maven.test.skip>\\n    </properties>\\n    <dependencies>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-web-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.jms</groupId>\\n            <artifactId>jboss-jms-api_2.0_spec</artifactId>\\n            <version>2.0.0.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.flywaydb</groupId>\\n            <artifactId>flyway-core</artifactId>\\n            <version>4.1.2</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.rmi</groupId>\\n            <artifactId>jboss-rmi-api_1.0_spec</artifactId>\\n            <version>1.0.2.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-resteasy</artifactId>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-arc</artifactId>\\n        </dependency>\\n    </dependencies>\\n    <build>\\n        <finalName>ROOT</finalName>\\n        <plugins>\\n            <plugin>\\n                <artifactId>maven-compiler-plugin</artifactId>\\n                <version>3.0</version>\\n                <configuration>\\n                    <encoding>${project.encoding}</encoding>\\n                    <source>1.8</source>\\n                    <target>1.8</target>\\n                </configuration>\\n            </plugin>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-war-plugin</artifactId>\\n                <version>3.2.0</version>\\n            </plugin>\\n        </plugins>\\n    </build>\\n    <profiles>\\n<!-- TODO: Add OpenShift profile here -->\\n    </profiles>\\n</project>```', name='read', id='09a31109-46b8-46d9-b9aa-80811271b287', tool_call_id='call_CMMZUntPQWq19jAWuOUfgK3S')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9QO4sa50zqe4nb3Rovy5LNml', 'function': {'arguments': '{\"file_name\":\"/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml\",\"content\":\"<?xml version=\\\\\"1.0\\\\\" encoding=\\\\\"UTF-8\\\\\"?>\\\\n<project \\\\n    xmlns=\\\\\"http://maven.apache.org/POM/4.0.0\\\\\" \\\\n    xmlns:xsi=\\\\\"http://www.w3.org/2001/XMLSchema-instance\\\\\" xsi:schemaLocation=\\\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\\\">\\\\n    <modelVersion>4.0.0</modelVersion>\\\\n    <groupId>com.redhat.coolstore</groupId>\\\\n    <artifactId>monolith</artifactId>\\\\n    <version>1.0.0-SNAPSHOT</version>\\\\n    <packaging>war</packaging>\\\\n    <name>coolstore-monolith</name>\\\\n    <properties>\\\\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\\\n        <maven.build.timestamp.format>yyyyMMdd\\'T\\'HHmmss</maven.build.timestamp.format>\\\\n        <project.encoding>UTF-8</project.encoding>\\\\n        <maven.test.skip>true</maven.test.skip>\\\\n    </properties>\\\\n    <dependencies>\\\\n        <dependency>\\\\n            <groupId>javax</groupId>\\\\n            <artifactId>javaee-web-api</artifactId>\\\\n            <version>7.0</version>\\\\n            <scope>provided</scope>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>javax</groupId>\\\\n            <artifactId>javaee-api</artifactId>\\\\n            <version>7.0</version>\\\\n            <scope>provided</scope>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>org.jboss.spec.javax.jms</groupId>\\\\n            <artifactId>jboss-jms-api_2.0_spec</artifactId>\\\\n            <version>2.0.0.Final</version>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>org.flywaydb</groupId>\\\\n            <artifactId>flyway-core</artifactId>\\\\n            <version>4.1.2</version>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>org.jboss.spec.javax.rmi</groupId>\\\\n            <artifactId>jboss-rmi-api_1.0_spec</artifactId>\\\\n            <version>1.0.2.Final</version>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>io.quarkus</groupId>\\\\n            <artifactId>quarkus-resteasy</artifactId>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>io.quarkus</groupId>\\\\n            <artifactId>quarkus-arc</artifactId>\\\\n        </dependency>\\\\n        <dependency>\\\\n            <groupId>io.quarkus</groupId>\\\\n            <artifactId>quarkus-smallrye-reactive-messaging</artifactId>\\\\n        </dependency>\\\\n    </dependencies>\\\\n    <build>\\\\n        <finalName>ROOT</finalName>\\\\n        <plugins>\\\\n            <plugin>\\\\n                <artifactId>maven-compiler-plugin</artifactId>\\\\n                <version>3.0</version>\\\\n                <configuration>\\\\n                    <encoding>${project.encoding}</encoding>\\\\n                    <source>1.8</source>\\\\n                    <target>1.8</target>\\\\n                </configuration>\\\\n            </plugin>\\\\n            <plugin>\\\\n                <groupId>org.apache.maven.plugins</groupId>\\\\n                <artifactId>maven-war-plugin</artifactId>\\\\n                <version>3.2.0</version>\\\\n            </plugin>\\\\n        </plugins>\\\\n    </build>\\\\n    <profiles>\\\\n<!-- TODO: Add OpenShift profile here -->\\\\n    </profiles>\\\\n</project>\"}', 'name': 'write'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 887, 'prompt_tokens': 1318, 'total_tokens': 2205, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2c8f7fb4-9736-486e-adaa-80f377ea8e5d-0', tool_calls=[{'name': 'write', 'args': {'file_name': '/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml', 'content': '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project \\n    xmlns=\"http://maven.apache.org/POM/4.0.0\" \\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n    <groupId>com.redhat.coolstore</groupId>\\n    <artifactId>monolith</artifactId>\\n    <version>1.0.0-SNAPSHOT</version>\\n    <packaging>war</packaging>\\n    <name>coolstore-monolith</name>\\n    <properties>\\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\n        <maven.build.timestamp.format>yyyyMMdd\\'T\\'HHmmss</maven.build.timestamp.format>\\n        <project.encoding>UTF-8</project.encoding>\\n        <maven.test.skip>true</maven.test.skip>\\n    </properties>\\n    <dependencies>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-web-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.jms</groupId>\\n            <artifactId>jboss-jms-api_2.0_spec</artifactId>\\n            <version>2.0.0.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.flywaydb</groupId>\\n            <artifactId>flyway-core</artifactId>\\n            <version>4.1.2</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.rmi</groupId>\\n            <artifactId>jboss-rmi-api_1.0_spec</artifactId>\\n            <version>1.0.2.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-resteasy</artifactId>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-arc</artifactId>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-smallrye-reactive-messaging</artifactId>\\n        </dependency>\\n    </dependencies>\\n    <build>\\n        <finalName>ROOT</finalName>\\n        <plugins>\\n            <plugin>\\n                <artifactId>maven-compiler-plugin</artifactId>\\n                <version>3.0</version>\\n                <configuration>\\n                    <encoding>${project.encoding}</encoding>\\n                    <source>1.8</source>\\n                    <target>1.8</target>\\n                </configuration>\\n            </plugin>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-war-plugin</artifactId>\\n                <version>3.2.0</version>\\n            </plugin>\\n        </plugins>\\n    </build>\\n    <profiles>\\n<!-- TODO: Add OpenShift profile here -->\\n    </profiles>\\n</project>'}, 'id': 'call_9QO4sa50zqe4nb3Rovy5LNml', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1318, 'output_tokens': 887, 'total_tokens': 2205, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='Successfully wrote contents to file /Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml', name='write', id='f6c43341-a1b0-4389-a76b-8f27ed9c7642', tool_call_id='call_9QO4sa50zqe4nb3Rovy5LNml')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gxoYegKLZjF5028xSCAcaf08', 'function': {'arguments': '{\"file_name_pattern\":\"**/application.properties\"}', 'name': 'find'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 2236, 'total_tokens': 2255, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2176}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7d0d9984-af24-4d94-a365-18d9fd5947a3-0', tool_calls=[{'name': 'find', 'args': {'file_name_pattern': '**/application.properties'}, 'id': 'call_gxoYegKLZjF5028xSCAcaf08', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2236, 'output_tokens': 19, 'total_tokens': 2255, 'input_token_details': {'audio': 0, 'cache_read': 2176}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='No files found matching pattern **/application.properties', name='find', id='ba81a777-ecfd-4798-91a6-ad1ddffc393a', tool_call_id='call_gxoYegKLZjF5028xSCAcaf08')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ci5Ftp9FCJEH9iR30AtpuxgV', 'function': {'arguments': '{\"file_name\":\"/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/src/main/resources/application.properties\",\"content\":\"# Configure the \\'orders\\' channel for SmallRye Reactive Messaging\\\\nmp.messaging.incoming.orders.connector=smallrye-kafka\\\\nmp.messaging.incoming.orders.topic=orders-topic\\\\nmp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer\\\\n\\\\nmp.messaging.outgoing.orders.connector=smallrye-kafka\\\\nmp.messaging.outgoing.orders.topic=processed-orders-topic\\\\nmp.messaging.outgoing.orders.value.serializer=org.apache.kafka.common.serialization.StringSerializer\\\\n\"}', 'name': 'write'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 2270, 'total_tokens': 2404, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2176}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2226bc66-5f0e-44db-a21b-7a8ac3986d4a-0', tool_calls=[{'name': 'write', 'args': {'file_name': '/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/src/main/resources/application.properties', 'content': \"# Configure the 'orders' channel for SmallRye Reactive Messaging\\nmp.messaging.incoming.orders.connector=smallrye-kafka\\nmp.messaging.incoming.orders.topic=orders-topic\\nmp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer\\n\\nmp.messaging.outgoing.orders.connector=smallrye-kafka\\nmp.messaging.outgoing.orders.topic=processed-orders-topic\\nmp.messaging.outgoing.orders.value.serializer=org.apache.kafka.common.serialization.StringSerializer\\n\"}, 'id': 'call_ci5Ftp9FCJEH9iR30AtpuxgV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2270, 'output_tokens': 134, 'total_tokens': 2404, 'input_token_details': {'audio': 0, 'cache_read': 2176}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='Successfully wrote contents to file /Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/src/main/resources/application.properties', name='write', id='99f76e9b-53a2-4cd3-bc4b-32e5c13da969', tool_call_id='call_ci5Ftp9FCJEH9iR30AtpuxgV')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='I have successfully added the necessary Quarkus SmallRye Reactive Messaging dependency to your `pom.xml`. Additionally, I\\'ve created the `application.properties` file to configure the messaging channel named `\"orders\"` to connect to a message broker using Kafka. If there are further configurations or migrations needed, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 2437, 'total_tokens': 2502, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-85105987-f022-41ef-9080-dde64eb5cf0e-0', usage_metadata={'input_tokens': 2437, 'output_tokens': 65, 'total_tokens': 2502, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "\n",
    "@tool\n",
    "def find(file_name_pattern: Annotated[str, \"Glob pattern to match file names\"]) -> Annotated[str, \"Absolute paths of the files found separated by newlines\"]:\n",
    "    \"\"\"Given a glob pattern, searches for files matching the pattern in project and returns absolute paths of the files\n",
    "\n",
    "    Returns:\n",
    "        str: Newline separated list of filepaths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        matching_files = glob.glob(\"./project/\" + file_name_pattern, recursive=True)\n",
    "    except Exception as e:\n",
    "        return f\"Failed to search for files because of error - {e}\"\n",
    "    absolute_paths = [os.path.abspath(file) for file in matching_files]\n",
    "    if not absolute_paths:\n",
    "        return f\"No files found matching pattern {file_name_pattern}\"\n",
    "    return \"\\n\".join(absolute_paths)\n",
    "\n",
    "@tool\n",
    "def read(file_name: Annotated[str, \"Absolute path to the file to read\"]):\n",
    "    \"\"\"Given an absolute path to a file, reads the file and returns content\"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        contents = \"\"\n",
    "        with open(file_name, \"r\") as f:\n",
    "            contents = f.read()\n",
    "        return f\"Here are the file contents:\\n```{contents}```\"\n",
    "    return f\"File {file_name} does not exist\"\n",
    "\n",
    "@tool\n",
    "def write(file_name: Annotated[str, \"Absolute path to the file\"], content: Annotated[str, \"Content to write to the file\"]):\n",
    "    \"\"\"Given an absolute path to a file and the content, writes content to the file. If the file doesn't exist, creates a new file.\n",
    "    If the directories dont exist, creates all parent directories.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        with open(file_name, \"w+\") as f:\n",
    "            f.write(content)\n",
    "        return f\"Successfully wrote contents to file {file_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to write to file with error {e}\"\n",
    "\n",
    "@tool\n",
    "def rm(file: Annotated[str, \"Full path of the file to remove\"]):\n",
    "    \"\"\"Removes file\"\"\"\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        return \"Removed the file\"\n",
    "    return f\"File {file} does not exist\"\n",
    "\n",
    "file_ops_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[find, read, write, rm],\n",
    "    prompt=\"\"\"You are an expert Java developer. You are asked to make changes in a Java project.\n",
    "You can either update an existing file, delete a file, or create a new file with relevant content.\n",
    "To update or delete an existing file, search for the file to update using a glob pattern, and determine which file to update from the files found.\n",
    "To update an existing file, make sure you read the file to get existing contents, and then write to the file updated content.\n",
    "If a file doesn't exist and needs to be created, create that file.\n",
    "Always use absolute file paths to make changes to files. You are given tools to perform file operations. Use the tools, and make the changes needed.\n",
    "Use your best judgement to solve the issues described.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "for state in file_ops_agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=additional_info_ex1)]},\n",
    "    stream_mode=[\"updates\"]\n",
    "):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In the above output, you can see that the model searched for `pom.xml` file and added the dependency. \n",
    "\n",
    "It also searched for `application.properties` file but it wasn't found, so it created a new file at path `src/main/resources` which is the correct path for Quarkus applications. \n",
    "\n",
    "This proves the possibility of updating another file from the project, AND creating a new file.\n",
    "\n",
    "I have attached screenshots of the results from my environment below.\n",
    "\n",
    "![pom_xml_update](./screenshots/pom_xml_update.png)\n",
    "\n",
    "![application_properties_creation](./screenshots/application_properties_creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Removal Example\n",
    "\n",
    "Lets take a look at another example where we have to remove a `web.xml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BErckULSix64WOTimSabFREt', 'function': {'arguments': '{\"file_name_pattern\":\"**/beans.xml\"}', 'name': 'find'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 486, 'total_tokens': 505, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e6e94688-0c50-4a39-b16d-ff9e3cb17734-0', tool_calls=[{'name': 'find', 'args': {'file_name_pattern': '**/beans.xml'}, 'id': 'call_BErckULSix64WOTimSabFREt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 486, 'output_tokens': 19, 'total_tokens': 505, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/beans.xml', name='find', id='f190cbae-6ddc-41a9-a2ab-afd992938b0b', tool_call_id='call_BErckULSix64WOTimSabFREt')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fhQHwDs0ZhAW9zGLUSfZZ8Wm', 'function': {'arguments': '{\"file\":\"/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/beans.xml\"}', 'name': 'rm'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 531, 'total_tokens': 564, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-34a04dd6-b1d1-4884-a318-587bdaded097-0', tool_calls=[{'name': 'rm', 'args': {'file': '/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/beans.xml'}, 'id': 'call_fhQHwDs0ZhAW9zGLUSfZZ8Wm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 531, 'output_tokens': 33, 'total_tokens': 564, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='Removed the file', name='rm', id='4f8bef7a-7ff1-4e97-aacd-40e8918d896d', tool_call_id='call_fhQHwDs0ZhAW9zGLUSfZZ8Wm')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gDENcPLEM8eJMewCkxPACMeA', 'function': {'arguments': '{\"file_name_pattern\":\"**/pom.xml\"}', 'name': 'find'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 573, 'total_tokens': 592, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-27b77fa0-2213-405d-861e-7bcd531e5eba-0', tool_calls=[{'name': 'find', 'args': {'file_name_pattern': '**/pom.xml'}, 'id': 'call_gDENcPLEM8eJMewCkxPACMeA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 573, 'output_tokens': 19, 'total_tokens': 592, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml', name='find', id='12f5f85f-437c-4ff1-832c-0c671efbf8a8', tool_call_id='call_gDENcPLEM8eJMewCkxPACMeA')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_hStsAd7uxMxG7kHaaH2S9YcM', 'function': {'arguments': '{\"file_name\":\"/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml\"}', 'name': 'read'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 618, 'total_tokens': 652, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e22505ba-69d0-4e9d-9ecb-7e041d36f2dd-0', tool_calls=[{'name': 'read', 'args': {'file_name': '/Users/pgaikwad/Projects/kai/notebooks/followup_agent/project/pom.xml'}, 'id': 'call_hStsAd7uxMxG7kHaaH2S9YcM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 618, 'output_tokens': 34, 'total_tokens': 652, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "('updates', {'tools': {'messages': [ToolMessage(content='Here are the file contents:\\n```<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project \\n    xmlns=\"http://maven.apache.org/POM/4.0.0\" \\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n    <groupId>com.redhat.coolstore</groupId>\\n    <artifactId>monolith</artifactId>\\n    <version>1.0.0-SNAPSHOT</version>\\n    <packaging>war</packaging>\\n    <name>coolstore-monolith</name>\\n    <properties>\\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\n        <maven.build.timestamp.format>yyyyMMdd\\'T\\'HHmmss</maven.build.timestamp.format>\\n        <project.encoding>UTF-8</project.encoding>\\n        <maven.test.skip>true</maven.test.skip>\\n    </properties>\\n    <dependencies>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-web-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>javax</groupId>\\n            <artifactId>javaee-api</artifactId>\\n            <version>7.0</version>\\n            <scope>provided</scope>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.jms</groupId>\\n            <artifactId>jboss-jms-api_2.0_spec</artifactId>\\n            <version>2.0.0.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.flywaydb</groupId>\\n            <artifactId>flyway-core</artifactId>\\n            <version>4.1.2</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.jboss.spec.javax.rmi</groupId>\\n            <artifactId>jboss-rmi-api_1.0_spec</artifactId>\\n            <version>1.0.2.Final</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-resteasy</artifactId>\\n        </dependency>\\n        <dependency>\\n            <groupId>io.quarkus</groupId>\\n            <artifactId>quarkus-arc</artifactId>\\n        </dependency>\\n    </dependencies>\\n    <build>\\n        <finalName>ROOT</finalName>\\n        <plugins>\\n            <plugin>\\n                <artifactId>maven-compiler-plugin</artifactId>\\n                <version>3.0</version>\\n                <configuration>\\n                    <encoding>${project.encoding}</encoding>\\n                    <source>1.8</source>\\n                    <target>1.8</target>\\n                </configuration>\\n            </plugin>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-war-plugin</artifactId>\\n                <version>3.2.0</version>\\n            </plugin>\\n        </plugins>\\n    </build>\\n    <profiles>\\n<!-- TODO: Add OpenShift profile here -->\\n    </profiles>\\n</project>```', name='read', id='bec5ee37-5afe-4480-9514-197deda81ef0', tool_call_id='call_hStsAd7uxMxG7kHaaH2S9YcM')]}})\n",
      "('updates', {'agent': {'messages': [AIMessage(content=\"The `pom.xml` file already includes the necessary CDI dependencies for Quarkus support, namely `quarkus-resteasy` and `quarkus-arc`. \\n\\nHere's a summary of the changes made and the current status:\\n- **Removed `beans.xml`:** The `beans.xml` file was successfully removed.\\n- **Verified Dependencies:** The `pom.xml` file contains the `quarkus-resteasy` and `quarkus-arc` extensions, ensuring CDI support in the application.\\n\\nNext, ensure that the application is tested properly to verify that CDI is functioning as expected without the `beans.xml` file. Additionally, refer to the [Quarkus CDI Guide](https://quarkus.io/guides/cdi) for more details on handling CDI with Quarkus.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 1395, 'total_tokens': 1561, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c8f1d8e-d7c5-4925-a4a9-80a2dc3bbfe7-0', usage_metadata={'input_tokens': 1395, 'output_tokens': 166, 'total_tokens': 1561, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n"
     ]
    }
   ],
   "source": [
    "additional_info_ex2 = \"\"\"- **Documentation Reference:** For more details on how Quarkus handles CDI, refer to the [Quarkus CDI Guide](https://quarkus.io/guides/cdi).\n",
    "- **Testing:** After removing `beans.xml`, ensure that the application is tested to verify that CDI is functioning as expected without the file.\n",
    "- **Dependencies:** Double-check the `pom.xml` to ensure all necessary CDI dependencies are included for Quarkus. This typically involves ensuring that the `quarkus-resteasy` or `quarkus-arc` extensions are present, as they provide CDI support.\"\"\"\n",
    "\n",
    "for state in file_ops_agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=additional_info_ex2)]},\n",
    "    stream_mode=[\"updates\"]\n",
    "):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can see that the `beans.xml` file is removed by the agent. See screenshot below:\n",
    "\n",
    "![beans_removal](./screenshots/beans_xml_removal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating tool usage with different providers via Model Provider\n",
    "\n",
    "This section validates that we are able to use tools with different Model Providers we support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "\n",
    "@tool\n",
    "def get_temperature(city: Annotated[str, \"Name of the city\"]):\n",
    "    \"\"\"Returns current temperature in given city\"\"\"\n",
    "    return f\"Current temperature in {city} is 80 degrees.\"\n",
    "\n",
    "def validate(model: LanguageModelLike, **kwargs):\n",
    "    agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=[get_temperature],\n",
    "        prompt=\"Help the user with their queries, use the tools at your disposal.\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    for r in agent.stream({\"messages\": [(\"user\", \"What is the current temperature in New York?\")]}):\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which model you test this with, you need to set up your API keys for the model before running the \"validation\" cell. Every cell has a comment at top about the environment variable expected.\n",
    "\n",
    "You will set the keys in `.env` file at the project root and run the following cell for changes to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|   Kai Model Provider   |      Model       |  Works?  |\n",
    "| ---------------------- | ---------------- | -------- |\n",
    "| ChatOpenAI             | gpt-3.5-turbo    |  &check; |\n",
    "| ChatOpenAI (RH Maas)   | granite-8b       |  &check; |\n",
    "| ChatOpenAI (RH Maas)   | llama-7b         |  &check; |\n",
    "| ChatBedrock            | llama-70b-v1     |  &cross; |\n",
    "| ChatBedrock            | llama-70b-v2     |  &cross; |\n",
    "| ChatBedrock            | claude-3-sonnet  |  &check; |\n",
    "| ChatGoogleGenAI        | gemini-2.0-flash |  &check; |\n",
    "| ChatDeepSeek           | deepseek-chat    |  &check; |\n",
    "\n",
    "Following model providers are not tested yet:\n",
    "\n",
    "* ChatOllama\n",
    "* ChatAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-93d38e3ef7764368aeab44edfa14db42', 'function': {'arguments': '{\"city\": \"New York\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 140, 'total_tokens': 165, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'granite-3-8b-instruct', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-97df9c64-fad3-4df2-b314-76b4fc50324e-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': 'chatcmpl-tool-93d38e3ef7764368aeab44edfa14db42', 'type': 'tool_call'}], usage_metadata={'input_tokens': 140, 'output_tokens': 25, 'total_tokens': 165, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='92aee6a7-fc0c-4137-991f-dfbdbd915f92', tool_call_id='chatcmpl-tool-93d38e3ef7764368aeab44edfa14db42')]}}\n",
      "{'agent': {'messages': [AIMessage(content='The current temperature in New York is 80 degrees.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 161, 'total_tokens': 174, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'granite-3-8b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-efec3793-40ed-4125-96ea-8dfdb247961a-0', usage_metadata={'input_tokens': 161, 'output_tokens': 13, 'total_tokens': 174, 'input_token_details': {}, 'output_token_details': {}})]}}\n"
     ]
    }
   ],
   "source": [
    "## Test with granite-8b on RH maas via ChatOpenAI\n",
    "## Env vars needed:\n",
    "## PARASOL_GRANITE_KEY - api key\n",
    "## PARASON_GRANITE_API - base url (usually ends with /v1)\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatOpenAI\n",
    "\n",
    "validate(ChatOpenAI(\n",
    "    api_key=os.environ[\"PARASOL_GRANITE_KEY\"],\n",
    "    base_url=os.environ[\"PARASOL_GRANITE_API\"],\n",
    "    model_name=\"granite-3-8b-instruct\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-671b7256d3e94bed833385429a6d5168', 'function': {'arguments': '{\"city\": \"New York\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 206, 'total_tokens': 224, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fe619e78-3d5f-4814-8a78-a4f9b24d9b7d-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': 'chatcmpl-tool-671b7256d3e94bed833385429a6d5168', 'type': 'tool_call'}], usage_metadata={'input_tokens': 206, 'output_tokens': 18, 'total_tokens': 224, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='ba529951-2d5f-4a2c-82ef-1dcf4bbac5fd', tool_call_id='chatcmpl-tool-671b7256d3e94bed833385429a6d5168')]}}\n",
      "{'agent': {'messages': [AIMessage(content=\"I used the hypothetical output of the function to create a sample response. However, in a real scenario, the function would return the actual temperature value as per the actual output of the 'get_temperature' function.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 249, 'total_tokens': 292, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-496d77f3-4914-46a8-90ac-225c3c7b2011-0', usage_metadata={'input_tokens': 249, 'output_tokens': 43, 'total_tokens': 292, 'input_token_details': {}, 'output_token_details': {}})]}}\n"
     ]
    }
   ],
   "source": [
    "validate(ChatOpenAI(\n",
    "    api_key=os.environ[\"PARASOL_LLAMA_KEY\"],\n",
    "    base_url=os.environ[\"PARASOL_LLAMA_API\"],\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_hyQWUzmMBoWcy7lgjdtKMqKK', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 75, 'total_tokens': 91, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ef88bd74-8e49-47b8-a583-ad33145a439f-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': 'call_hyQWUzmMBoWcy7lgjdtKMqKK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 75, 'output_tokens': 16, 'total_tokens': 91, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='7339a847-6267-4964-8f81-0c3ca9a3c4e0', tool_call_id='call_hyQWUzmMBoWcy7lgjdtKMqKK')]}}\n",
      "{'agent': {'messages': [AIMessage(content='The current temperature in New York is 80 degrees.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 108, 'total_tokens': 121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-176cd009-bd24-40c6-be5c-dd29a95bfd59-0', usage_metadata={'input_tokens': 108, 'output_tokens': 13, 'total_tokens': 121, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "## Test with gpt-3.5-turbo via ChatOpenAI\n",
    "## Env vars needed:\n",
    "## OPENAI_API_KEY - api key\n",
    "\n",
    "validate(ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_temperature', 'arguments': '{\"city\": \"New York\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0345ec89-e933-4a1a-9645-ca54d4a3d3b6-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': '84239db4-b642-4c92-9a83-ec759f341dff', 'type': 'tool_call'}], usage_metadata={'input_tokens': 38, 'output_tokens': 6, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='5ed17fd0-c2b7-4f5a-9449-0c1287060c19', tool_call_id='84239db4-b642-4c92-9a83-ec759f341dff')]}}\n",
      "{'agent': {'messages': [AIMessage(content='The current temperature in New York is 80 degrees.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-9e6ccb8d-9110-4cee-94db-a6085ffffed8-0', usage_metadata={'input_tokens': 59, 'output_tokens': 13, 'total_tokens': 72, 'input_token_details': {'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "## Test with gemini-2.0-flash via GoogleGenAI\n",
    "## Env vars needed:\n",
    "## GEMINI_API_KEY - api key\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatGoogleGenerativeAI\n",
    "\n",
    "validate(ChatGoogleGenerativeAI(\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    model=\"gemini-2.0-flash\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I'm just an AI, I don't have access to real-time weather data. However, I can suggest some ways for you to find out the current temperature in New York:\\n\\n1. Check online weather websites: You can check websites like AccuWeather, Weather.com, or the National Weather Service (NWS) for the current temperature in New York.\\n2. Use a weather app: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, which can provide you with real-time weather information, including the current temperature.\\n3. Check social media: Many weather organizations and meteorologists share current weather conditions, including temperature, on social media platforms like Twitter and Facebook.\\n4. Tune into local news: You can watch local news or listen to local radio stations to get the current temperature and weather forecast for New York.\\n\\nPlease note that the temperature can change rapidly, so it's always a good idea to check multiple sources for the most up-to-date information.\", additional_kwargs={'usage': {'prompt_tokens': 39, 'completion_tokens': 200, 'total_tokens': 239}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-1-70b-instruct-v1:0'}, response_metadata={'usage': {'prompt_tokens': 39, 'completion_tokens': 200, 'total_tokens': 239}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-1-70b-instruct-v1:0'}, id='run-e5b9bdad-42ca-4706-87d3-45b911b4d674-0', usage_metadata={'input_tokens': 39, 'output_tokens': 200, 'total_tokens': 239})]}}\n"
     ]
    }
   ],
   "source": [
    "## Test with llama-70b via Bedrock\n",
    "## Env vars needed:\n",
    "## AWS_ACCESS_KEY_ID\n",
    "## AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatBedrock\n",
    "\n",
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I'm just an AI, I don't have real-time access to current weather conditions. However, I can suggest some ways for you to find out the current temperature in New York.\\n\\nYou can check the current weather forecast for New York on websites like:\\n\\n* National Weather Service (NWS) - [www.weather.gov](http://www.weather.gov)\\n* AccuWeather - [www.accuweather.com](http://www.accuweather.com)\\n* Weather.com - [www.weather.com](http://www.weather.com)\\n* Google Maps - [maps.google.com](http://maps.google.com) (look for the weather icon)\\n\\nYou can also check your local news or weather app on your smartphone for the current temperature in New York.\\n\\nIf you're looking for a more specific answer, I can try to help you with that. What city in New York are you looking for the temperature in?\", additional_kwargs={'usage': {'prompt_tokens': 39, 'completion_tokens': 182, 'total_tokens': 221}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-2-1b-instruct-v1:0'}, response_metadata={'usage': {'prompt_tokens': 39, 'completion_tokens': 182, 'total_tokens': 221}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-2-1b-instruct-v1:0'}, id='run-43bcbfbb-00fb-4616-ab8b-b1aaadee2f32-0', usage_metadata={'input_tokens': 39, 'output_tokens': 182, 'total_tokens': 221})]}}\n"
     ]
    }
   ],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-2-1b-instruct-v1:0\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='I\\'m just an AI, I don\\'t have access to real-time information or current weather conditions. But I can suggest some ways for you to find out the current temperature in New York:\\n\\n1. Check online weather websites: You can check websites like AccuWeather, Weather.com, or the National Weather Service (NWS) for current weather conditions and temperature in New York.\\n2. Use a search engine: You can simply type \"current temperature in New York\" or \"New York weather\" in a search engine like Google, and it will show you the current temperature and weather conditions.\\n3. Check a weather app: If you have a smartphone, you can download a weather app like Dark Sky or Weather Underground, which can provide you with current weather conditions and temperature in New York.\\n4. Check social media: You can also check social media platforms like Twitter or Facebook for updates on the current weather in New York.\\n\\nPlease note that the temperature in New York can vary depending on the time of day, location, and other factors, so it\\'s always a good idea to check multiple sources for the most up-to-date information.', additional_kwargs={'usage': {'prompt_tokens': 40, 'completion_tokens': 229, 'total_tokens': 269}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-3-70b-instruct-v1:0'}, response_metadata={'usage': {'prompt_tokens': 40, 'completion_tokens': 229, 'total_tokens': 269}, 'stop_reason': 'stop', 'model_id': 'us.meta.llama3-3-70b-instruct-v1:0'}, id='run-8d93f8a0-197f-4851-88b4-a494678c7d59-0', usage_metadata={'input_tokens': 40, 'output_tokens': 229, 'total_tokens': 269})]}}\n"
     ]
    }
   ],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 253, 'completion_tokens': 54, 'total_tokens': 307}, 'stop_reason': 'tool_use', 'model_id': 'us.anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 253, 'completion_tokens': 54, 'total_tokens': 307}, 'stop_reason': 'tool_use', 'model_id': 'us.anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-ebe41a8a-c51b-4c47-b222-b73af17499b4-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': 'toolu_bdrk_01ACM8hMxuQHJeGnbQLDqqgz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 54, 'total_tokens': 307})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='51824bd5-9d8e-4e26-8795-603c48d155c6', tool_call_id='toolu_bdrk_01ACM8hMxuQHJeGnbQLDqqgz')]}}\n",
      "{'agent': {'messages': [AIMessage(content='The current temperature in New York is 80 degrees Fahrenheit.', additional_kwargs={'usage': {'prompt_tokens': 329, 'completion_tokens': 19, 'total_tokens': 348}, 'stop_reason': 'end_turn', 'model_id': 'us.anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 329, 'completion_tokens': 19, 'total_tokens': 348}, 'stop_reason': 'end_turn', 'model_id': 'us.anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-128488f9-58ac-4baf-99a8-41e9b40bf676-0', usage_metadata={'input_tokens': 329, 'output_tokens': 19, 'total_tokens': 348})]}}\n"
     ]
    }
   ],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0_1211c9dd-2ad1-43f3-8696-94c912284b06', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_temperature'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 129, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 128}, 'prompt_cache_hit_tokens': 128, 'prompt_cache_miss_tokens': 1}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-06857c74-a5ef-4121-a4a5-590df9397845-0', tool_calls=[{'name': 'get_temperature', 'args': {'city': 'New York'}, 'id': 'call_0_1211c9dd-2ad1-43f3-8696-94c912284b06', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 20, 'total_tokens': 149, 'input_token_details': {'cache_read': 128}, 'output_token_details': {}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Current temperature in New York is 80 degrees.', name='get_temperature', id='b4290fe8-05a3-474c-b7a9-5d005ec45cda', tool_call_id='call_0_1211c9dd-2ad1-43f3-8696-94c912284b06')]}}\n",
      "{'__interrupt__': ()}\n"
     ]
    }
   ],
   "source": [
    "# Env vars needed:\n",
    "# DEEPSEEK_API_KEY\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatDeepSeek\n",
    "\n",
    "validate(model=ChatDeepSeek(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "    model=\"deepseek-chat\",\n",
    "), interrupt_after=[\"tools\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tools with models that do not support tool calling\n",
    "\n",
    "In this section, we experiment creating our own _LangGraph ToolNode_ and custom logic in the _Conditional Edge_ to enable models that don't support tools to call given tools. Note that we saw examples of models in previous sections that do not support tools out of the box. It is absolutely essential that we are able to use tools with any model if we have to use LangGraph approach for agents. We validate whether its possible to do that in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## we use meta-llama-70b provided via Bedrock in this section\n",
    "\n",
    "import json\n",
    "from typing import Optional, TypedDict\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, SystemMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain.tools.render import render_text_description\n",
    "from kai.llm_interfacing.model_provider import ChatBedrock\n",
    "\n",
    "def _print_state(state: MessagesState):\n",
    "    print(\"...\"*15)\n",
    "    for idx, message in enumerate(state[\"messages\"]):\n",
    "        print(f\"{idx}: {message.content.splitlines()[0] if message.content else message.content}\")\n",
    "    print(\"...\"*15)\n",
    "\n",
    "@tool\n",
    "def get_temperature(city: Annotated[str, \"Name of the city\"]):\n",
    "    \"\"\"Returns current temperature in given city\"\"\"\n",
    "    return f\"Current temperature in {city} is 80 degrees.\"\n",
    "\n",
    "# define the tool node\n",
    "tool_node = ToolNode(tools=[get_temperature])\n",
    "\n",
    "# define the model provider\n",
    "model_provider = ChatBedrock(\n",
    "    model=\"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    ")\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extracts JSON code block from a multiline string.\"\"\"\n",
    "    pattern = r\"```(json)?\\s*(\\{.*?\\})\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(2)\n",
    "        try:\n",
    "            return json.loads(json_str)  # Convert to Python dict\n",
    "        except json.JSONDecodeError:\n",
    "            return None  # Invalid JSON format\n",
    "\n",
    "    return None\n",
    "\n",
    "# define the agent\n",
    "def agent_call(state: MessagesState):\n",
    "    sys_msg = f\"\"\"You are an intelligent assistant who can help users with their weather related queries. \n",
    "You may not know answers to all questions as some of the answers may depend on real time weather data. However, you are given tools you can use to access that data.\n",
    "Here is the schema of tools you are given:\n",
    "\n",
    "{render_text_description([get_temperature])}\n",
    "\n",
    "If you do need to call a tool, respond with a JSON object containing only two keys - tool_name and args. `tool_name` should be the name of the tool to call and `args` should be nested JSON containing the arguments to pass to the function in key value format.\n",
    "Make sure you always use ``` at the start and end of the JSON block to clearly separate it from text.\n",
    "If you have reached the answer, respond with text and add string 'FINAL ANSWER' to the beginning.\n",
    "\"\"\"\n",
    "    if \"messages\" not in state or not state[\"messages\"] or \\\n",
    "        not isinstance(state[\"messages\"][0], SystemMessage):\n",
    "        state[\"messages\"].insert(0, SystemMessage(content=sys_msg))\n",
    "    if isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        tool_output = state[\"messages\"][-1]\n",
    "        state[\"messages\"][-1] = HumanMessage(\n",
    "            content=f\"The output of tool {tool_output.name} is - {tool_output.content}\"\n",
    "        )\n",
    "    response = model_provider.invoke(state[\"messages\"])\n",
    "    if \"tool_name\" in response.content:\n",
    "        agent_response = extract_json_from_text(response.content)\n",
    "        if not agent_response:\n",
    "            return { \"messages\": [AIMessage(content=\"RETRY\")] }\n",
    "        response = AIMessage(\n",
    "            content=\"\",\n",
    "            tool_calls=[{\n",
    "                \"name\": agent_response.get(\"tool_name\"),\n",
    "                \"args\": agent_response.get(\"args\"),\n",
    "                \"type\": \"tool_call\",\n",
    "                \"id\": \"tool_call_id_1\",\n",
    "            }],\n",
    "        )\n",
    "    return { \"messages\": [response] }\n",
    "\n",
    "# this is the conditional edge to parse and\n",
    "# route the llm response to a tool\n",
    "def should_call_tool(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    if not last_message.content or \"RETRY\" in last_message.content:\n",
    "        return \"agent\"\n",
    "    return END\n",
    "    \n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent_call)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_call_tool, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Human Message =================================\n",
      "\n",
      "what's the weather in New York?\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  get_temperature (tool_call_id_1)\n",
      " Call ID: tool_call_id_1\n",
      "  Args:\n",
      "    city: New York\n",
      "================================= Tool Message =================================\n",
      "Name: get_temperature\n",
      "\n",
      "Current temperature in New York is 80 degrees.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got unknown type content='Current temperature in New York is 80 degrees.' name='get_temperature' id='e9e1a4e4-e60e-496b-b736-8afb0b8dac79' tool_call_id='tool_call_id_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather in New York?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1993\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1993\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   2000\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[177], line 60\u001b[0m, in \u001b[0;36magent_call\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], SystemMessage):\n\u001b[1;32m     59\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, SystemMessage(content\u001b[38;5;241m=\u001b[39msys_msg))\n\u001b[0;32m---> 60\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_provider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m     62\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m extract_json_from_text(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock.py:563\u001b[0m, in \u001b[0;36mChatBedrock._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             system \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt_with_tools\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptAdapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_messages_to_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop:\n\u001b[1;32m    568\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock.py:350\u001b[0m, in \u001b[0;36mChatPromptAdapter.convert_messages_to_prompt\u001b[0;34m(cls, provider, messages, model)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[0;32m--> 350\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_messages_to_prompt_llama3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m convert_messages_to_prompt_llama(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock.py:110\u001b[0m, in \u001b[0;36mconvert_messages_to_prompt_llama3\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_messages_to_prompt_llama3\u001b[39m(messages: List[BaseMessage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a list of messages to a prompt for llama.\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    109\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|begin_of_text|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[43m_convert_one_message_to_text_llama3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock.py:100\u001b[0m, in \u001b[0;36m_convert_one_message_to_text_llama3\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     96\u001b[0m     message_text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>system\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_text\n",
      "\u001b[0;31mValueError\u001b[0m: Got unknown type content='Current temperature in New York is 80 degrees.' name='get_temperature' id='e9e1a4e4-e60e-496b-b736-8afb0b8dac79' tool_call_id='tool_call_id_1'",
      "\u001b[0mDuring task with name 'agent' and id '145afcf9-e8b7-e46b-363f-a721cfae45b0'"
     ]
    }
   ],
   "source": [
    "for chunk in app.stream(\n",
    "    {\"messages\": [(\"human\", \"what's the weather in New York?\")]}, stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk[\"messages\"][-1].pretty_repr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
