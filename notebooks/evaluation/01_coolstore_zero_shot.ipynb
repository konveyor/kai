{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coolstore Evaluation\n",
    "\n",
    "This notebook will work with LLM to provide fixes for issues identified in [Coolstore](https://github.com/konveyor-ecosystem/coolstore) app for different kind of incidents. The incidents have varying levels of complexity for the fixes.  For each incident, we try to evaluate the responses under two conditions - with and without supplemental information from the analysis. We do at least 3 experiments with a prompt before concluding.\n",
    "\n",
    "Find incidents used in this experiment [here](./analysis_output.yaml).\n",
    "\n",
    "For comparing with expected output, we will use already modernized version of coolstore app for Quarkus which can be found in `quarkus` branch of the repo [here](https://github.com/konveyor-ecosystem/coolstore/tree/quarkus).\n",
    "\n",
    "For evaluating the responses, we will use different approaches based on complexity of a fix in question. For easy fixes, we will use [evaluation.py](../../kai/evaluation.py) which uses edit distance. For more complex fixes, we explore using another LLM. For determining consistency of responses, we will use standard deviation of the edit distance.\n",
    "\n",
    "_You will need to activate and use virtualenv to run snippets in this notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 src/main/webapp/WEB-INF/web.xml\n",
      "12 pom.xml\n",
      "10 src/main/java/com/redhat/coolstore/model/Order.java\n",
      "6 src/main/java/com/redhat/coolstore/model/OrderItem.java\n",
      "5 src/main/webapp/WEB-INF/beans.xml\n",
      "8 src/main/resources/META-INF/persistence.xml\n",
      "6 src/main/java/com/redhat/coolstore/model/InventoryEntity.java\n",
      "1 src/main/java/com/redhat/coolstore/model/ShoppingCart.java\n",
      "6 src/main/java/com/redhat/coolstore/persistence/Resources.java\n",
      "9 src/main/java/com/redhat/coolstore/rest/CartEndpoint.java\n",
      "8 src/main/java/com/redhat/coolstore/rest/OrderEndpoint.java\n",
      "3 src/main/java/com/redhat/coolstore/rest/ProductEndpoint.java\n",
      "4 src/main/java/com/redhat/coolstore/rest/RestApplication.java\n",
      "8 src/main/java/com/redhat/coolstore/service/CatalogService.java\n",
      "2 src/main/java/com/redhat/coolstore/service/InventoryNotificationMDB.java\n",
      "8 src/main/java/com/redhat/coolstore/service/OrderService.java\n",
      "15 src/main/java/com/redhat/coolstore/service/OrderServiceMDB.java\n",
      "3 src/main/java/com/redhat/coolstore/service/ProductService.java\n",
      "1 src/main/java/com/redhat/coolstore/service/PromoService.java\n",
      "4 src/main/java/com/redhat/coolstore/service/ShippingService.java\n",
      "10 src/main/java/com/redhat/coolstore/service/ShoppingCartOrderProcessor.java\n",
      "3 src/main/java/com/redhat/coolstore/service/ShoppingCartService.java\n",
      "7 src/main/java/com/redhat/coolstore/utils/DataBaseMigrationStartup.java\n",
      "3 src/main/java/com/redhat/coolstore/utils/Producers.java\n",
      "1 src/main/java/com/redhat/coolstore/utils/StartupListener.java\n",
      "6 src/main/java/com/redhat/coolstore/utils/Transformers.java\n"
     ]
    }
   ],
   "source": [
    "# first we group incidents by files\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../kai')\n",
    "from kai.models.report import Report\n",
    "\n",
    "output_file = './analysis_output.yaml'\n",
    "report = Report.load_report_from_file(output_file)\n",
    "files = report.get_impacted_files()\n",
    "\n",
    "# we filter out filepaths for dependencies\n",
    "to_delete = []\n",
    "for k in files: \n",
    "    if k.startswith('root/.m2'): to_delete.append(k)\n",
    "for d in to_delete: del(files[d])\n",
    "\n",
    "# printing file names and incidents in each file\n",
    "for f in files: print(len(files[f]), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the files displayed above, we will be focusing on following files in our experiments:\n",
    "\n",
    "* src/main/java/com/redhat/coolstore/model/ShoppingCart.java\n",
    "* src/main/java/com/redhat/coolstore/model/InventoryEntity.java\n",
    "* src/main/java/com/redhat/coolstore/service/CatalogService.java\n",
    "* src/main/java/com/redhat/coolstore/service/ShippingService.java\n",
    "* src/main/java/com/redhat/coolstore/service/ShoppingCartOrderProcessor.java\n",
    "\n",
    "These files appear in our demo example found [here](https://github.com/konveyor/kai/blob/main/docs/scenarios/demo.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will get our test data\n",
    "import os\n",
    "import errno\n",
    "from git import Repo\n",
    "import importlib\n",
    "\n",
    "def ensure_dirs(dir):\n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def clone_coolstore(branch: str, path: str):\n",
    "    try:\n",
    "        Repo.clone_from(\"https://github.com/konveyor-ecosystem/coolstore\", \n",
    "            depth=1, single_branch=True, branch=branch, to_path=path)\n",
    "    except Exception as e:\n",
    "        if \"already exists\" not in str(e):\n",
    "            print(\"fatal error cloning repo\")\n",
    "            sys.exit(1)\n",
    "\n",
    "ensure_dirs(\"./data/apps/coolstore/\")\n",
    "clone_coolstore(\"quarkus\", \"./data/apps/coolstore/quarkus\")\n",
    "clone_coolstore(\"main\", \"./data/apps/coolstore/javaee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create data required for evaluation\n",
    "from datetime import datetime\n",
    "from kai.evaluation import BenchmarkExample, evaluate, BenchmarkResult\n",
    "from kai.service.incident_store import Application\n",
    "\n",
    "examples = {}\n",
    "for f in files:\n",
    "    original_content = \"\"\n",
    "    expected_content = \"\"\n",
    "    with open(f\"./data/apps/coolstore/javaee/{f}\", \"r\") as fl: original_content = fl.read()\n",
    "    if os.path.exists(f\"./data/apps/coolstore/quarkus/{f}\"): \n",
    "        with open(f\"./data/apps/coolstore/quarkus/{f}\", \"r\") as fl: expected_content = fl.read()\n",
    "    examples[f] = BenchmarkExample(\n",
    "        application=Application(\n",
    "            application_name=\"coolstore\",\n",
    "            current_branch=\"main\",\n",
    "            repo_uri_local=\"./data/apps/coolstore/javaee\",\n",
    "            generated_at=datetime.strptime(\"24/05/09 19:32:00\", \"%y/%m/%d %H:%M:%S\"),\n",
    "            repo_uri_origin=\"https://github.com/konveyor-ecosystem/coolstore\",\n",
    "            current_commit=\"aa\"\n",
    "        ),\n",
    "        expected_file=expected_content,\n",
    "        incidents=files[f],\n",
    "        original_file=original_content,\n",
    "        name=os.path.basename(f),\n",
    "        report=report,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "CONFIG_BASE_PATH = \"./data/configs/\"\n",
    "OUTPUT_BASE_PATH = \"./data/outputs/\"\n",
    "\n",
    "ensure_dirs(CONFIG_BASE_PATH)\n",
    "ensure_dirs(OUTPUT_BASE_PATH)\n",
    "\n",
    "templ = Template(\"\"\"\n",
    "trace_enabled = true\n",
    "demo_mode = false\n",
    "log_dir = \"$pwd/logs\"\n",
    "file_log_level = \"debug\"\n",
    "log_level = \"info\"\n",
    "\n",
    "[models]\n",
    "provider = \"{{ model_provider }}\"\n",
    "template = \"{{ prompt_template }}\"\n",
    "\n",
    "[models.args]\n",
    "model_id = \"{{ model_id }}\"\n",
    "{% if max_tokens != \"\" %}\n",
    "parameters.max_new_tokens = \"{{ max_tokens }}\"\n",
    "{% endif %}\n",
    "\n",
    "[incident_store]\n",
    "solution_detectors = \"naive\"\n",
    "solution_producers = \"text_only\"\n",
    "\n",
    "[incident_store.args]\n",
    "provider = \"postgresql\"\n",
    "host = \"127.0.0.1\"\n",
    "database = \"kai\"\n",
    "user = \"kai\"\n",
    "password = \"dog8code\"\n",
    "\"\"\")\n",
    "\n",
    "# some shorthands we can use in our experiments for different models\n",
    "IBM_LLAMA_13b = 'ibm-llama-13b'\n",
    "IBM_LLAMA_70b = 'ibm-llama-70b'\n",
    "IBM_MIXTRAL = 'ibm-mixtral'\n",
    "IBM_GRANITE = 'ibm-granite'\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_3 = \"gpt-3\"\n",
    "\n",
    "# model_provider: { model_id: {parameter: val}}\n",
    "models = {\n",
    "    \"ChatIBMGenAI\": {\n",
    "        \"meta-llama/llama-3-70b-instruct\": {\"max_tokens\": \"2048\", \"key\": IBM_LLAMA_70b},\n",
    "        \"meta-llama/llama-2-13b-chat\": {\"max_tokens\": \"1536\", \"key\": IBM_LLAMA_13b},\n",
    "        \"mistralai/mixtral-8x7b-instruct-v01\": {\"key\": IBM_MIXTRAL},\n",
    "        \"ibm/granite-13b-chat-v2\": {\"key\": IBM_GRANITE},\n",
    "    },\n",
    "    \"ChatOpenAI\": {\n",
    "        \"gpt-3.5-turbo\": {\"key\": GPT_4},\n",
    "        \"gpt-4\": {\"key\": GPT_3},\n",
    "    },\n",
    "}\n",
    "\n",
    "configs = {}\n",
    "\n",
    "# create configs for all models with different parameters \n",
    "# we will use these as needed in our experiments\n",
    "for model_provider, model_ids in models.items():\n",
    "    for model_id, parameters in model_ids.items():\n",
    "        configs[parameters.get(\"key\", \"\")] = Template(templ.render(\n",
    "            model_provider = model_provider,\n",
    "            model_id = model_id,\n",
    "            max_tokens = parameters.get(\"max_tokens\", \"\"),\n",
    "            prompt_template = \"{{ prompt_template }}\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is common code we will use to evaluate response of LLM for one example\n",
    "import json\n",
    "import signal\n",
    "import requests\n",
    "import threading\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from kai.models.kai_config import KaiConfig\n",
    "from kai.routes.get_incident_solutions_for_file import (\n",
    "    PostGetIncidentSolutionsForFileParams,\n",
    ")\n",
    "\n",
    "# function to calculate edit distance between two strings\n",
    "def levenshtein_distance(s1, s2) -> float:\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2 + 1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(\n",
    "                    1 + min((distances[i1], distances[i1 + 1], distances_[-1]))\n",
    "                )\n",
    "        distances = distances_\n",
    "    return float(distances[-1])\n",
    "\n",
    "# helper function to send requests to Kai service\n",
    "def generate_fix(log: any, params: PostGetIncidentSolutionsForFileParams) -> dict:\n",
    "    retries_left = 6\n",
    "    for i in range(retries_left):\n",
    "        try:\n",
    "            headers = {\"Content-type\": \"application/json\", \"Accept\": \"text/plain\"}\n",
    "            response = requests.post(\n",
    "                \"http://0.0.0.0:8080/get_incident_solutions_for_file\",\n",
    "                data=params.model_dump_json(),\n",
    "                headers=headers,\n",
    "                timeout=3600,\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if isinstance(result, str):\n",
    "                    return json.loads(result)\n",
    "                elif isinstance(result, dict):\n",
    "                    return result\n",
    "                else:\n",
    "                    return {}\n",
    "            else:\n",
    "                log.write(f\"[{params.file_name}] Received status code {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log.write(f\"[{params.file_name}] Received exception from Kai server: {e}\")\n",
    "        log.write(f\"[{params.file_name}] Failed to get a '200' response from the server.  Retrying {retries_left-i} more times\")\n",
    "    raise(Exception(f\"[{params.file_name}] Failed to get a '200' response from the server.  Parameters = {params}\"))\n",
    "\n",
    "\n",
    "# write a Kai config to a known location \n",
    "def ensure_config(model_key: str, experiment_key: str, prompt_template: str) -> tuple[str, str]:\n",
    "    ensure_dirs(f\"{CONFIG_BASE_PATH}{experiment_key}\")\n",
    "    config_path = f\"{CONFIG_BASE_PATH}{experiment_key}/{model_key}.toml\"\n",
    "    config = configs[model_key].render(prompt_template=prompt_template)\n",
    "    with open(config_path, \"w+\") as f: f.write(config)\n",
    "    config_parsed = KaiConfig.model_validate_filepath(config_path)\n",
    "    return config_path, config_parsed\n",
    "\n",
    "# run Kai service, this is used when we use Kai service instead of evaluate.py for generating fixes\n",
    "def ensure_kai_service(output_path: str) -> tuple[list, list]:\n",
    "    processes = []\n",
    "    def run_command(cmd: str, stdout: any): \n",
    "        p = subprocess.Popen(cmd, shell=True, cwd=\"../../\", \n",
    "            env=os.environ.copy(), stdout=stdout, stderr=stdout)\n",
    "        processes.append(p)\n",
    "        p.wait()\n",
    "    ensure_dirs(f\"{output_path}\")\n",
    "    postgres_log = open(f\"{output_path}/postgres.log\", \"w+\")\n",
    "    db_thread  = threading.Thread(target=run_command, args=(\"DROP_TABLES=true POSTGRES_RUN_ARGS=--rm make run-postgres\", postgres_log, ))\n",
    "    kai_log = open(f\"{output_path}/kai.log\", \"w+\")\n",
    "    kai_thread = threading.Thread(target=run_command, args=(\"make run-server\", kai_log, ))\n",
    "    db_thread.start()\n",
    "    data_load_log = open(f\"{output_path}/data_load.log\", \"w+\")\n",
    "    subprocess.run([\"make\", \"load-data\"], cwd=\"../../\", stdout=data_load_log, stderr=data_load_log)\n",
    "    kai_thread.start()\n",
    "    return processes, [kai_log, postgres_log, data_load_log]\n",
    "\n",
    "# helper function to kill processes gracefully, needed to clean up Kai service and db\n",
    "def kill(processes: list):\n",
    "    for p in processes:\n",
    "        p.send_signal(signal.SIGINT)\n",
    "        p.send_signal(signal.SIGTERM)\n",
    "\n",
    "# this function runs \"evaluate\" function from evaluation.py and compares LLM responses with expected output to get edit distance\n",
    "def run_evaluate_for_example(model_key: str, experiment_key: str, prompt_template: str, example: BenchmarkExample):\n",
    "    config_path, config_parsed = ensure_config(model_key, experiment_key, prompt_template)\n",
    "    full_response = evaluate(configs={config_path: config_parsed}, examples={example.name: example})\n",
    "    response = full_response[(example.name, config_path)]\n",
    "    output_path = f\"{OUTPUT_BASE_PATH}/using_evaluation/{experiment_key}/{model_key}\"\n",
    "    ensure_dirs(output_path)\n",
    "    with open(f\"{output_path}/llm_response\", \"w+\") as f: f.write(response.llm_result)\n",
    "    with open(f\"{output_path}/edit_distance\", \"w+\") as f: f.write(f\"{response.similarity}\")\n",
    "    with open(f\"{output_path}/updated_file\", \"w+\") as f: f.write(f\"{response.updated_file}\")\n",
    "\n",
    "# this function sends example in a query to Kai service to get LLM response\n",
    "def run_kai_generate_fix(model_key: str, experiment_key: str, prompt_template: str, example: BenchmarkExample):\n",
    "    output_path = f\"{OUTPUT_BASE_PATH}/using_kai/{experiment_key}/{model_key}\"\n",
    "    ensure_dirs(output_path)\n",
    "    runner_log = open(f\"{output_path}/runner.log\", \"w+\")\n",
    "    config_path, config_parsed = ensure_config(model_key, experiment_key, os.path.basename(prompt_template))\n",
    "    subprocess.run(['cp', f'../../kai/config.toml', f'../../kai/config.toml.temp'])\n",
    "    subprocess.run(['cp', f'{config_path}', f'../../kai/config.toml'])\n",
    "    subprocess.run(['cp', f'{prompt_template}', f'../../kai/data/templates/'])\n",
    "    processes, logs = ensure_kai_service(output_path=output_path)\n",
    "    # sleep is needed to let Kai server come up\n",
    "    sleep(10)\n",
    "    file_contents = \"\"\n",
    "    try:\n",
    "        with open(example.original_file, \"r\") as f: file_contents = f.read()\n",
    "        params = PostGetIncidentSolutionsForFileParams(\n",
    "            application_name=example.application.application_name,\n",
    "            file_contents=example.original_file,\n",
    "            file_name=example.name,\n",
    "            include_llm_results=False,\n",
    "            incidents=example.incidents,\n",
    "        )\n",
    "        response = generate_fix(runner_log, params)\n",
    "        if not response: \n",
    "            raise(f\"failed to parse response\")\n",
    "        with open(f\"{output_path}/llm_reasoning\", \"w+\") as f: f.write(response.get('total_reasoning', [''])[0])\n",
    "        with open(f\"{output_path}/prompt\", \"w+\") as f: f.write(response.get('used_prompts', [''])[0])\n",
    "        with open(f\"{output_path}/updated_file\", \"w+\") as f: f.write(response.get('updated_file', ['']))\n",
    "    except Exception as e:\n",
    "        runner_log.write(f\"failed to generate fix {e}\")\n",
    "    finally:\n",
    "        # clean up\n",
    "        subprocess.run(['rm', f'../../kai/data/templates/{os.path.basename(prompt_template)}'])\n",
    "        subprocess.run(['mv', f'../../kai/config.toml.temp', f'../../kai/config.toml'])\n",
    "        kill(processes)\n",
    "        # sleep is needed to gracefully shutdown\n",
    "        sleep(10)\n",
    "        runner_log.close()\n",
    "        for log in logs: log.close()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# make sure you set GENAI_KEY / OPENAI_KEY\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot with No Analysis Information\n",
    "\n",
    "In this section, we will evaluate performance for an easy fix. This fix simply requires updating import statements. We will run this with different models and will not provide any analysis information in the prompt. The prompt we will use can be found [here](./templates/zero_shot/example1.jinja). Since this fix is easy, we will use edit distance to understand accuracy of responses.\n",
    "\n",
    "We will run the following cell to run the easy example with all 4 IBM models. The outputs of this experiment will be generated in `./data/outputs/using_kai/zero_shot_easy/` directory for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file only contains fixes that require changing imports...an easy example\n",
    "example = examples['src/main/java/com/redhat/coolstore/model/ShoppingCart.java']\n",
    "\n",
    "# run with all models and plot graphs for edit distance and consistency\n",
    "for model in [IBM_LLAMA_13b, IBM_LLAMA_70b, IBM_GRANITE, IBM_MIXTRAL]:\n",
    "    response = run_kai_generate_fix(model, \"zero_shot_easy\", \"./templates/zero_shot/example1.jinja\", example)\n",
    "    # response = run_evaluate_for_example(model, \"zero_shot_easy\", \"./templates/zero_shot/example1.jinja\", example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ibm-llama-13b': 751.0, 'ibm-llama-70b': 625.0, 'ibm-granite': 1461.0, 'ibm-mixtral': 45.0}\n"
     ]
    }
   ],
   "source": [
    "# Now we will compute and plot the edit distance of responses\n",
    "data = {}\n",
    "for model in [IBM_LLAMA_13b, IBM_LLAMA_70b, IBM_GRANITE, IBM_MIXTRAL]:\n",
    "    # get updated file\n",
    "    base_output_path = \"./data/outputs/using_kai/zero_shot_easy\"\n",
    "    updated_content = \"\"\n",
    "    with open(f\"{base_output_path}/{model}/updated_file\", \"r\") as f: updated_content = f.read()\n",
    "    data[model] = levenshtein_distance(example.expected_file, updated_content)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
