{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coolstore Evaluation\n",
    "\n",
    "This notebook will work with LLM to provide fixes for issues identified in [Coolstore](https://github.com/konveyor-ecosystem/coolstore) app for different kind of incidents. The incidents have varying levels of complexity for the fixes.  For each incident, we try to evaluate the responses under two conditions - with and without supplemental information from the analysis. We do at least 3 experiments with a prompt before concluding.\n",
    "\n",
    "Find incidents used in this experiment [here](./analysis_output.yaml).\n",
    "\n",
    "For comparing with expected output, we will use already modernized version of coolstore app for Quarkus which can be found in `quarkus` branch of the repo [here](https://github.com/konveyor-ecosystem/coolstore/tree/quarkus).\n",
    "\n",
    "For evaluating the responses, we will use different approaches based on complexity of a fix in question. For easy fixes, we will use [evaluation.py](../../kai/evaluation.py) which uses edit distance. For more complex fixes, we explore using another LLM. For determining consistency of responses, we will use standard deviation of the edit distance.\n",
    "\n",
    "_You will need to activate and use virtualenv to run snippets in this notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 src/main/webapp/WEB-INF/web.xml\n",
      "12 pom.xml\n",
      "10 src/main/java/com/redhat/coolstore/model/Order.java\n",
      "6 src/main/java/com/redhat/coolstore/model/OrderItem.java\n",
      "5 src/main/webapp/WEB-INF/beans.xml\n",
      "8 src/main/resources/META-INF/persistence.xml\n",
      "6 src/main/java/com/redhat/coolstore/model/InventoryEntity.java\n",
      "1 src/main/java/com/redhat/coolstore/model/ShoppingCart.java\n",
      "6 src/main/java/com/redhat/coolstore/persistence/Resources.java\n",
      "9 src/main/java/com/redhat/coolstore/rest/CartEndpoint.java\n",
      "8 src/main/java/com/redhat/coolstore/rest/OrderEndpoint.java\n",
      "3 src/main/java/com/redhat/coolstore/rest/ProductEndpoint.java\n",
      "4 src/main/java/com/redhat/coolstore/rest/RestApplication.java\n",
      "8 src/main/java/com/redhat/coolstore/service/CatalogService.java\n",
      "2 src/main/java/com/redhat/coolstore/service/InventoryNotificationMDB.java\n",
      "8 src/main/java/com/redhat/coolstore/service/OrderService.java\n",
      "15 src/main/java/com/redhat/coolstore/service/OrderServiceMDB.java\n",
      "3 src/main/java/com/redhat/coolstore/service/ProductService.java\n",
      "1 src/main/java/com/redhat/coolstore/service/PromoService.java\n",
      "4 src/main/java/com/redhat/coolstore/service/ShippingService.java\n",
      "10 src/main/java/com/redhat/coolstore/service/ShoppingCartOrderProcessor.java\n",
      "3 src/main/java/com/redhat/coolstore/service/ShoppingCartService.java\n",
      "7 src/main/java/com/redhat/coolstore/utils/DataBaseMigrationStartup.java\n",
      "3 src/main/java/com/redhat/coolstore/utils/Producers.java\n",
      "1 src/main/java/com/redhat/coolstore/utils/StartupListener.java\n",
      "6 src/main/java/com/redhat/coolstore/utils/Transformers.java\n"
     ]
    }
   ],
   "source": [
    "# first we group incidents by files\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../kai')\n",
    "from kai.models.report import Report\n",
    "\n",
    "output_file = './analysis_output.yaml'\n",
    "report = Report.load_report_from_file(output_file)\n",
    "files = report.get_impacted_files()\n",
    "\n",
    "# we filter out filepaths for dependencies\n",
    "to_delete = []\n",
    "for k in files: \n",
    "    if k.startswith('root/.m2'): to_delete.append(k)\n",
    "for d in to_delete: del(files[d])\n",
    "\n",
    "# printing file names and incidents in each file\n",
    "for f in files: print(len(files[f]), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the files displayed above, we will be focusing on following files in our experiments:\n",
    "\n",
    "* src/main/java/com/redhat/coolstore/model/ShoppingCart.java\n",
    "* src/main/java/com/redhat/coolstore/model/InventoryEntity.java\n",
    "* src/main/java/com/redhat/coolstore/service/CatalogService.java\n",
    "* src/main/java/com/redhat/coolstore/service/ShippingService.java\n",
    "* src/main/java/com/redhat/coolstore/service/ShoppingCartOrderProcessor.java\n",
    "\n",
    "These files appear in our demo example found [here](https://github.com/konveyor/kai/blob/main/docs/scenarios/demo.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will get our test data\n",
    "import os\n",
    "import errno\n",
    "from git import Repo\n",
    "\n",
    "def ensure_dirs(dir):\n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def clone_coolstore(branch: str, path: str):\n",
    "    try:\n",
    "        Repo.clone_from(\"https://github.com/konveyor-ecosystem/coolstore\", \n",
    "            depth=1, single_branch=True, branch=branch, to_path=path)\n",
    "    except Exception as e:\n",
    "        if \"already exists\" not in str(e):\n",
    "            print(\"fatal error cloning repo\")\n",
    "            sys.exit(1)\n",
    "\n",
    "ensure_dirs(\"./data/apps/coolstore/\")\n",
    "clone_coolstore(\"quarkus\", \"./data/apps/coolstore/quarkus\")\n",
    "clone_coolstore(\"main\", \"./data/apps/coolstore/javaee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create data required for evaluation\n",
    "from datetime import datetime\n",
    "from kai.evaluation import BenchmarkExample, evaluate\n",
    "from kai.service.incident_store import Application\n",
    "\n",
    "examples = {}\n",
    "for f in files:\n",
    "    examples[f] = BenchmarkExample(\n",
    "        application=Application(\n",
    "            application_name=\"coolstore\",\n",
    "            current_branch=\"main\",\n",
    "            repo_uri_local=\"./data/apps/coolstore/javaee\",\n",
    "            generated_at=datetime.strptime(\"24/05/09 19:32:00\", \"%y/%m/%d %H:%M:%S\"),\n",
    "            repo_uri_origin=\"https://github.com/konveyor-ecosystem/coolstore\",\n",
    "            current_commit=\"aa\"\n",
    "        ),\n",
    "        expected_file=f\"./data/apps/coolstore/quarkus/{f}\",\n",
    "        incidents=files[f],\n",
    "        original_file=f\"./data/apps/coolstore/javaee/{f}\",\n",
    "        name=os.path.basename(f),\n",
    "        report=report,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "CONFIG_BASE_PATH = \"./data/configs/\"\n",
    "OUTPUT_BASE_PATH = \"./data/outputs/\"\n",
    "\n",
    "ensure_dirs(CONFIG_BASE_PATH)\n",
    "ensure_dirs(OUTPUT_BASE_PATH)\n",
    "\n",
    "templ = Template(\"\"\"\n",
    "trace_enabled = true\n",
    "demo_mode = false\n",
    "log_dir = \"$pwd/logs\"\n",
    "file_log_level = \"debug\"\n",
    "log_level = \"info\"\n",
    "\n",
    "[models]\n",
    "provider = \"{{ model_provider }}\"\n",
    "template = \"{{ prompt_template }}\"\n",
    "\n",
    "[models.args]\n",
    "model_id = \"{{ model_id }}\"\n",
    "{% if max_tokens != \"\" %}\n",
    "parameters.max_new_tokens = \"{{ max_tokens }}\"\n",
    "{% endif %}\n",
    "\n",
    "[incident_store]\n",
    "solution_detectors = \"naive\"\n",
    "solution_producers = \"text_only\"\n",
    "\n",
    "[incident_store.args]\n",
    "provider = \"postgresql\"\n",
    "host = \"127.0.0.1\"\n",
    "database = \"kai\"\n",
    "user = \"kai\"\n",
    "password = \"dog8code\"\n",
    "\"\"\")\n",
    "\n",
    "# some shorthands we can use in our experiments for different models\n",
    "IBM_LLAMA_13b = 'ibm-llama-13b'\n",
    "IBM_LLAMA_70b = 'ibm-llama-70b'\n",
    "IBM_MIXTRAL = 'ibm-mixtral'\n",
    "IBM_GRANITE = 'ibm-granite'\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_3 = \"gpt-3\"\n",
    "\n",
    "# model_provider: { model_id: {parameter: val}}\n",
    "models = {\n",
    "    \"ChatIBMGenAI\": {\n",
    "        \"meta-llama/llama-3-70b-instruct\": {\"max_tokens\": \"2048\", \"key\": IBM_LLAMA_70b},\n",
    "        \"meta-llama/llama-2-13b-chat\": {\"max_tokens\": \"1536\", \"key\": IBM_LLAMA_13b},\n",
    "        \"mistralai/mixtral-8x7b-instruct-v01\": {\"key\": IBM_MIXTRAL},\n",
    "        \"ibm/granite-13b-chat-v2\": {\"key\": IBM_GRANITE},\n",
    "    },\n",
    "    \"ChatOpenAI\": {\n",
    "        \"gpt-3.5-turbo\": {\"key\": GPT_4},\n",
    "        \"gpt-4\": {\"key\": GPT_3},\n",
    "    },\n",
    "}\n",
    "\n",
    "configs = {}\n",
    "\n",
    "# create configs for all models with different parameters \n",
    "# we will use these as needed in our experiments\n",
    "for model_provider, model_ids in models.items():\n",
    "    for model_id, parameters in model_ids.items():\n",
    "        configs[parameters.get(\"key\", \"\")] = Template(templ.render(\n",
    "            model_provider = model_provider,\n",
    "            model_id = model_id,\n",
    "            max_tokens = parameters.get(\"max_tokens\", \"\"),\n",
    "            prompt_template = \"{{ prompt_template }}\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is common code we will use to evaluate response of LLM for one example\n",
    "import signal\n",
    "import requests\n",
    "import threading\n",
    "import subprocess\n",
    "from kai.models.kai_config import KaiConfig\n",
    "from kai.routes.get_incident_solutions_for_file import (\n",
    "    PostGetIncidentSolutionsForFileParams,\n",
    ")\n",
    "\n",
    "# helper function to send requests to Kai service\n",
    "def _generate_fix(params: PostGetIncidentSolutionsForFileParams):\n",
    "    headers = {\"Content-type\": \"application/json\", \"Accept\": \"text/plain\"}\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:8080/get_incident_solutions_for_file\",\n",
    "        data=params.model_dump_json(),\n",
    "        headers=headers,\n",
    "        timeout=3600,\n",
    "    )\n",
    "    return response\n",
    "# helper function to send requests to Kai service\n",
    "def generate_fix(params: PostGetIncidentSolutionsForFileParams):\n",
    "    retries_left = 6\n",
    "    for i in range(retries_left):\n",
    "        try:\n",
    "            response = _generate_fix(params)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"[{params.file_name}] Received status code {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[{params.file_name}] Received exception: {e}\")\n",
    "        print(f\"[{params.file_name}] Failed to get a '200' response from the server.  Retrying {retries_left-i} more times\")\n",
    "    print(f\"[{params.file_name}] Failed to get a '200' response from the server.  Parameters = {params}\")\n",
    "\n",
    "# write a Kai config to a known location \n",
    "def ensure_config(model_key: str, experiment_key: str, prompt_template: str) -> tuple[str, str]:\n",
    "    ensure_dirs(f\"{CONFIG_BASE_PATH}{experiment_key}\")\n",
    "    config_path = f\"{CONFIG_BASE_PATH}{experiment_key}/{model_key}.toml\"\n",
    "    config = configs[model_key].render(prompt_template=prompt_template)\n",
    "    with open(config_path, \"w+\") as f: f.write(config)\n",
    "    config_parsed = KaiConfig.model_validate_filepath(config_path)\n",
    "    return config_path, config_parsed\n",
    "\n",
    "# run Kai service, this is used when we use Kai service instead of evaluate.py for generating fixes\n",
    "def ensure_kai_service(output_path: str) -> list:\n",
    "    processes = []\n",
    "    def run_command(cmd: str, stdout: any): \n",
    "        p = subprocess.Popen(cmd, shell=True, cwd=\"../../\", \n",
    "            env=os.environ.copy(), stdout=stdout, stderr=stdout)\n",
    "        processes.append(p)\n",
    "        p.wait()\n",
    "    ensure_dirs(f\"{output_path}\")\n",
    "    postgres_log = open(f\"{output_path}/postgres.log\", \"w+\")\n",
    "    db_thread  = threading.Thread(target=run_command, args=(\"DROP_TABLES=true POSTGRES_RUN_ARGS=--rm make run-postgres\", postgres_log, ))\n",
    "    kai_log = open(f\"{output_path}/kai.log\", \"w+\")\n",
    "    kai_thread = threading.Thread(target=run_command, args=(\"make run-server\", kai_log, ))\n",
    "    db_thread.start()\n",
    "    subprocess.run([\"make\", \"load-data\"], cwd=\"../../\")\n",
    "    kai_thread.start()\n",
    "    return processes\n",
    "\n",
    "# helper function to kill processes gracefully, needed to clean up Kai service and db\n",
    "def kill(processes: list):\n",
    "    for p in processes:\n",
    "        p.send_signal(signal.SIGINT)\n",
    "        p.send_signal(signal.SIGTERM)\n",
    "\n",
    "# this function runs \"evaluate\" function from evaluation.py and compares LLM responses with expected output to get edit distance\n",
    "def run_evaluate_for_example(model_key: str, experiment_key: str, prompt_template: str, example: BenchmarkExample):\n",
    "    config_path, config_parsed = ensure_config(model_key, experiment_key, prompt_template)\n",
    "    full_response = evaluate(configs={config_path: config_parsed}, examples={example.original_file: example})\n",
    "    response = full_response[(example.original_file, config_path)]\n",
    "    output_path = f\"{OUTPUT_BASE_PATH}{experiment_key}/{model_key}\"\n",
    "    ensure_dirs(output_path)\n",
    "    with open(f\"{output_path}/llm_response\", \"w+\") as f: f.write(response.llm_result)\n",
    "    with open(f\"{output_path}/edit_distance\", \"w+\") as f: f.write(f\"{response.similarity}\")\n",
    "\n",
    "# this function sends example in a query to Kai service to get LLM response\n",
    "def run_kai_response(model_key: str, experiment_key: str, prompt_template: str, example: BenchmarkExample):\n",
    "    config_path, config_parsed = ensure_config(model_key, experiment_key, f\"../notebooks/evaluation/{prompt_template}\")\n",
    "    subprocess.run(['cp', f'{config_path}', f'../../kai/config.toml'])\n",
    "    output_path = f\"{OUTPUT_BASE_PATH}{experiment_key}/{model_key}\"\n",
    "    processes = ensure_kai_service(output_path=output_path)\n",
    "    file_contents = \"\"\n",
    "    try:\n",
    "        with open(example.original_file, \"r\") as f: file_contents = f.read()\n",
    "        params = PostGetIncidentSolutionsForFileParams(\n",
    "            application_name=example.application.application_name,\n",
    "            file_contents=str(file_contents),\n",
    "            file_name=example.original_file,\n",
    "            include_llm_results=False,\n",
    "        )\n",
    "        response = generate_fix(params)\n",
    "        ensure_dirs(output_path)\n",
    "        print(response)\n",
    "    except:\n",
    "        print(\"failed to run evaluate\")\n",
    "    finally:\n",
    "        kill(processes)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you set GENAI_KEY / OPENAI_KEY\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot with No Analysis Information\n",
    "\n",
    "In this section, we will evaluate performance for an easy fix. This fix simply requires updating import statements. We will run this with different models and will not provide any analysis information in the prompt. The prompt we will use can be found [here](./templates/zero_shot/example1.jinja). Since this fix is easy, we will use edit distance to understand accuracy of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/outputs/zero_shot_easy/ibm-granite/postgres.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# run with all models and plot graphs for edit distance and consistency\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m [IBM_GRANITE, IBM_MIXTRAL, IBM_LLAMA_13b, IBM_LLAMA_70b]:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#response = run_evaluate_for_example(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#    model_key=model, experiment_key=\"zero_shot_easy\", \u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#    prompt_template=\"./templates/zero_shot/template1.jinja\", example=example)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_kai_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero_shot_easy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./templates/zero_shot/template1.jinja\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 83\u001b[0m, in \u001b[0;36mrun_kai_response\u001b[0;34m(model_key, experiment_key, prompt_template, example)\u001b[0m\n\u001b[1;32m     81\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../kai/config.toml\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     82\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_BASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexperiment_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m processes \u001b[38;5;241m=\u001b[39m \u001b[43mensure_kai_service\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m file_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[26], line 53\u001b[0m, in \u001b[0;36mensure_kai_service\u001b[0;34m(output_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     52\u001b[0m     p\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m---> 53\u001b[0m postgres_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/postgres.log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m db_thread  \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mrun_command, args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP_TABLES=true POSTGRES_RUN_ARGS=--rm make run-postgres\u001b[39m\u001b[38;5;124m\"\u001b[39m, postgres_log, ))\n\u001b[1;32m     55\u001b[0m kai_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/kai.log\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/kai/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/outputs/zero_shot_easy/ibm-granite/postgres.log'"
     ]
    }
   ],
   "source": [
    "# this file only contains import replaces as fixes needed, an easy example\n",
    "example = examples['src/main/java/com/redhat/coolstore/service/InventoryNotificationMDB.java']\n",
    "\n",
    "# run with all models and plot graphs for edit distance and consistency\n",
    "for model in [IBM_GRANITE, IBM_MIXTRAL, IBM_LLAMA_13b, IBM_LLAMA_70b]:\n",
    "    #response = run_evaluate_for_example(\n",
    "    #    model_key=model, experiment_key=\"zero_shot_easy\", \n",
    "    #    prompt_template=\"./templates/zero_shot/template1.jinja\", example=example)\n",
    "    \n",
    "    response = run_kai_response(model, \"zero_shot_easy\", \"./templates/zero_shot/template1.jinja\", example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
