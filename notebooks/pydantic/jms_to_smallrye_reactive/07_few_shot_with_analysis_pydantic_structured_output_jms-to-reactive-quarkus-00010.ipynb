{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot: JMS -> SmallRye Reactive - jms-to-reactive-quarkus-00010\n",
    "\n",
    "> Work with a LLM to generate a fix for the rule: jms-to-reactive-quarkus-00010\n",
    "\n",
    "We will include the solved example in this example to see if we get better quality results\n",
    "\n",
    "##### Sample Applications Used\n",
    "* 2 sample apps from [JBoss EAP Quickstarts](https://github.com/jboss-developer/jboss-eap-quickstarts/tree/7.4.x) were chosen:  helloworld-mdb & cmt\n",
    "    * [helloworld-mdb](https://github.com/savitharaghunathan/helloworld-mdb)\n",
    "    * [cmt](https://github.com/konveyor-ecosystem/cmt)\n",
    "\n",
    "##### Using Custom Rules for JMS to SmallRye Reactive\n",
    "* Rules were developed by Juanma [@jmle](https://github.com/jmle)\n",
    "    * Rules originally from: https://github.com/jmle/rulesets/blob/jms-rule/default/generated/quarkus/05-jms-to-reactive-quarkus.windup.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install local kai package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -e ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the common deps\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "\n",
    "def get_java_in_result(text: str):\n",
    "    _, after = text.split(\"```java\")\n",
    "    return after.split(\"```\")[0]\n",
    "\n",
    "\n",
    "def write_output_to_disk(\n",
    "    out_dir,\n",
    "    formatted_template,\n",
    "    example_javaee_file_contents,\n",
    "    example_quarkus_file_contents,\n",
    "):\n",
    "    try:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    except OSError as error:\n",
    "        print(f\"Error creating directory {out_dir}: {error}\")\n",
    "        raise error\n",
    "\n",
    "    # Save the template to disk\n",
    "    template_path = os.path.join(out_dir, \"template.txt\")\n",
    "    with open(template_path, \"w\") as f:\n",
    "        f.truncate()\n",
    "        f.write(formatted_template)\n",
    "    print(f\"Saved template to {template_path}\")\n",
    "\n",
    "    # Save the original source code\n",
    "    original_file_path = os.path.join(out_dir, \"original_file.java\")\n",
    "    with open(original_file_path, \"w\") as f:\n",
    "        f.truncate()\n",
    "        f.write(example_javaee_file_contents)\n",
    "    print(f\"Saved original java file to {original_file_path}\")\n",
    "\n",
    "    # Save the Quarkus version we already in Git to use as a comparison\n",
    "    known_quarkus_file_path = os.path.join(out_dir, \"quarkus_version_from_git.java\")\n",
    "    with open(known_quarkus_file_path, \"w\") as f:\n",
    "        f.truncate()\n",
    "        f.write(example_quarkus_file_contents)\n",
    "    print(f\"Saved Quarkus version from Git to {known_quarkus_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Prompt\n",
    "## Step #1:  Create a Prompt Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_solved_example_and_analysis = \"\"\"\n",
    "    # Java EE to Quarkus Migration\n",
    "    You are an AI Assistant trained on migrating enterprise JavaEE code to Quarkus.\n",
    "    I will give you an example of a JavaEE file and you will give me the Quarkus equivalent.\n",
    "\n",
    "    To help you update this file to Quarkus I will provide you with static source code analysis information\n",
    "    highlighting an issue which needs to be addressed, I will also provide you with an example of how a similar\n",
    "    issue was solved in the past via a solved example.  You can refer to the solved example for a pattern of\n",
    "    how to update the input Java EE file to Quarkus.\n",
    "\n",
    "    Be sure to pay attention to the issue found from static analysis and treat it as the primary issue you must \n",
    "    address or explain why you are unable to.\n",
    "\n",
    "    Approach this code migration from Java EE to Quarkus as if you were an experienced enterprise Java EE developer.\n",
    "    Before attempting to migrate the code to Quarkus, explain each step of your reasoning through what changes \n",
    "    are required and why. \n",
    "\n",
    "    Pay attention to changes you make and impacts to external dependencies in the pom.xml as well as changes \n",
    "    to imports we need to consider.\n",
    "\n",
    "    As you make changes that impact the pom.xml or imports, be sure you explain what needs to be updated.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "### Input:\n",
    "    ## Issue found from static code analysis of the Java EE code which needs to be fixed to migrate to Quarkus\n",
    "    Issue to fix:  \"{analysis_message}\"\n",
    "\n",
    "    ## Solved Example Filename\n",
    "    Filename: \"{solved_example_file_name}\"\n",
    "\n",
    "    ## Solved Example Git Diff \n",
    "    This diff of the solved example shows what changes we made in past to address a similar problem.\n",
    "    Please consider this heavily in your response.\n",
    "    ```diff\n",
    "    {solved_example_diff}\n",
    "    ```\n",
    "\n",
    "    ## Input file name\n",
    "    Filename: \"{src_file_name}\"\n",
    "\n",
    "    ## Input Line number of the issue first appearing in the Java EE code source code example below\n",
    "    Line number: {analysis_lineNumber}\n",
    "    \n",
    "    ## Input source code file contents for \"{src_file_name}\"\n",
    "    ```java \n",
    "    {src_file_contents}\n",
    "    ```\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Gather the data we want to inject into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading report from ./analysis_report/cmt/output.yaml\n",
      "Reading report from ./analysis_report/helloworld-mdb-quarkus/output.yaml\n"
     ]
    }
   ],
   "source": [
    "from kai.report import Report\n",
    "from kai.scm import GitDiff\n",
    "\n",
    "# Static code analysis was run prior and committed to this repo\n",
    "path_cmt_analysis = \"./analysis_report/cmt/output.yaml\"\n",
    "path_helloworld_analysis = \"./analysis_report/helloworld-mdb-quarkus/output.yaml\"\n",
    "\n",
    "cmt_report = Report.load_report_from_file(path_cmt_analysis)\n",
    "helloworld_report = Report.load_report_from_file(path_helloworld_analysis)\n",
    "\n",
    "# We are limiting our experiment to a single rule for this run\n",
    "ruleset_name = \"custom-ruleset\"\n",
    "rule = \"jms-to-reactive-quarkus-00010\"\n",
    "cmt_rule_data = cmt_report.report[ruleset_name][\"violations\"][rule]\n",
    "helloworld_rule_data = helloworld_report.report[ruleset_name][\"violations\"][rule]\n",
    "\n",
    "# We are expecting to find 1 impacted file in CMT and 2 impacted files in HelloWorld\n",
    "assert len(cmt_rule_data[\"incidents\"]), 1\n",
    "assert len(helloworld_rule_data[\"incidents\"]), 2\n",
    "\n",
    "# Setup access to look at the Git repo source code for each sample app\n",
    "cmt_src_path = \"../../../kai_solution_server/samples/sample_repos/cmt\"\n",
    "cmt_diff = GitDiff(cmt_src_path)\n",
    "\n",
    "# Ensure we checked out the 'quarkus' branch of the CMT repo\n",
    "cmt_diff.checkout_branch(\"quarkus\")\n",
    "\n",
    "helloworld_src_path = \"../../../kai_solution_server/samples/sample_repos/helloworld-mdb-quarkus\"\n",
    "helloworld_diff = GitDiff(helloworld_src_path)\n",
    "\n",
    "# We want to be sure the the 'quarkus' branch of both repos has been checked out\n",
    "# so it's available to consult\n",
    "cmt_diff.checkout_branch(\"quarkus\")\n",
    "helloworld_diff.checkout_branch(\"quarkus\")\n",
    "# Resetting to 'main' for HelloWorld as that represents th initial state of the repo\n",
    "helloworld_diff.checkout_branch(\"main\")\n",
    "\n",
    "# Now we can extract the info we need\n",
    "## We will assume:\n",
    "## .   HelloWorld will be our source application to migrate, so we will use it's 'main' branch which maps to Java EE\n",
    "## .   CMT will serve as our solved example, so we will consult it's 'quarkus' branch\n",
    "\n",
    "hw_entry = helloworld_rule_data[\"incidents\"][0]\n",
    "src_file_name = helloworld_report.get_cleaned_file_path(hw_entry[\"uri\"])\n",
    "src_file_contents = helloworld_diff.get_file_contents(src_file_name, \"main\")\n",
    "analysis_message = hw_entry[\"message\"]\n",
    "analysis_lineNumber = hw_entry[\"lineNumber\"]\n",
    "\n",
    "cmt_entry = cmt_rule_data[\"incidents\"][0]\n",
    "solved_example_file_name = cmt_report.get_cleaned_file_path(cmt_entry[\"uri\"])\n",
    "# solved_file_contents = cmt_diff.get_file_contents(solved_example_file_name, \"quarkus\")\n",
    "\n",
    "start_commit_id = cmt_diff.get_commit_from_branch(\"main\").hexsha\n",
    "end_commit_id = cmt_diff.get_commit_from_branch(\"quarkus\").hexsha\n",
    "\n",
    "solved_example_diff = cmt_diff.get_patch_for_file(\n",
    "    start_commit_id, end_commit_id, solved_example_file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run the prompt through the LLM and write the output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved template to output/openai_gpt-3.5-turbo-16k/few_shot_pydantic/template.txt\n",
      "Saved original java file to output/openai_gpt-3.5-turbo-16k/few_shot_pydantic/original_file.java\n",
      "Saved Quarkus version from Git to output/openai_gpt-3.5-turbo-16k/few_shot_pydantic/quarkus_version_from_git.java\n",
      "Saved result in output/openai_gpt-3.5-turbo-16k/few_shot_pydantic/result.txt\n"
     ]
    }
   ],
   "source": [
    "import caikit_tgis_langchain\n",
    "import traceback\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "\n",
    "class UpdatedFile(BaseModel):\n",
    "    file_name: str = Field(description=\"File name of udpated file\")\n",
    "    file_contents: str = Field(description=\"Contents of the updated file\")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: List[str] = Field(description=\"Process Explanation\")\n",
    "    updated_files: List[UpdatedFile] = Field(\n",
    "        description=\"List containing updated files\"\n",
    "    )\n",
    "\n",
    "\n",
    "response_parser = PydanticOutputParser(pydantic_object=Response)\n",
    "\n",
    "# For first run we will only include the source file and analysis info (we are omitting solved example)\n",
    "template_args = {\n",
    "    \"src_file_name\": src_file_name,\n",
    "    \"src_file_contents\": src_file_contents,\n",
    "    \"analysis_message\": analysis_message,\n",
    "    \"analysis_lineNumber\": analysis_lineNumber,\n",
    "    \"solved_example_file_name\": solved_example_file_name,\n",
    "    \"solved_example_diff\": solved_example_diff,\n",
    "    \"format_instructions\": response_parser.get_format_instructions(),\n",
    "}\n",
    "\n",
    "formatted_prompt = template_with_solved_example_and_analysis.format(**template_args)\n",
    "\n",
    "# For comparison, we will look up what the source file looks like from Quarkus branch\n",
    "# this serves as a way of comparing to what the 'answer' is that was done manually by a knowledgeable developer\n",
    "src_file_from_quarkus_branch = helloworld_diff.get_file_contents(\n",
    "    src_file_name, \"quarkus\"\n",
    ")\n",
    "\n",
    "### Choose one of the options below to run against\n",
    "### Uncomment the provider, one model, and the llm\n",
    "\n",
    "## OpenAI: Specify a model and make sure OPENAI_API_KEY is exported\n",
    "# provider = \"openai\"\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "# model = \"gpt-3.5-turbo-16k\"\n",
    "# model = \"gpt-4-1106-preview\"\n",
    "# llm = ChatOpenAI(temperature=0.1,\n",
    "#                 model_name=model,\n",
    "#                 streaming=True)\n",
    "\n",
    "## Oobabooga: text-gen uses the OpenAI API so it works similarly.\n",
    "## We load a model and specify temperature on the server so we don't need to specify them.\n",
    "## We've had decent results with Mistral-7b and Codellama-34b-Instruct\n",
    "## Just the URL for the server and make sure OPENAI_API_KEY is exported\n",
    "# provider = \"text-gen\"\n",
    "# model = \"CodeLlama-7b-Instruct-hf\"\n",
    "# model = \"CodeLlama-13b-Instruct-hf\"\n",
    "# model = \"CodeLlama-34b-Instruct-hf\"\n",
    "# model = \"CodeLlama-70b-Instruct-hf\"\n",
    "# model = \"starcoder\"\n",
    "# model = \"starcoder2-3b\"\n",
    "# model = \"starcoder2-7b\"\n",
    "# model = \"starcoder2-15b\"\n",
    "# model = \"WizardCoder-15B-V1.0\"\n",
    "# model = \"Mistral-7B-v0.1\"\n",
    "# model = \"Mixtral-8x7B-v0.1\"\n",
    "# llm = ChatOpenAI(openai_api_base=\"https://text-gen-api-text-gen.apps.ai.migration.redhat.com/v1\",\n",
    "#                 temperature=0.1,\n",
    "#                 streaming=True)\n",
    "\n",
    "## OpenShift AI:\n",
    "## We need to make sure caikit-nlp-client is installed.\n",
    "## As well as caikit_tgis_langchain.py: https://github.com/rh-aiservices-bu/llm-on-openshift/blob/main/examples/notebooks/langchain/caikit_tgis_langchain.py\n",
    "## Then set the model_id and server url\n",
    "## But we are having issues with responses: https://github.com/opendatahub-io/caikit-nlp-client/issues/95\n",
    "# provider = \"openshift-ai\"\n",
    "# model = \"codellama-7b-hf\"\n",
    "# llm = caikit_tgis_langchain.CaikitLLM(inference_server_url=\"https://codellama-7b-hf-predictor-kyma-workshop.apps.ai.migration.redhat.com\",\n",
    "#                                      model_id=\"CodeLlama-7b-hf\",\n",
    "#                                      streaming=True)\n",
    "\n",
    "## IBM Models:\n",
    "## Ensure GENAI_KEY is exported. Change the model if desired.\n",
    "# provider = \"ibm\"\n",
    "# model = \"ibm/granite-13b-instruct-v1\"\n",
    "# model = \"ibm/granite-13b-instruct-v2\"\n",
    "# model = \"ibm/granite-20b-5lang-instruct-rc\"\n",
    "# model = \"ibm/granite-20b-code-instruct-v1\"\n",
    "# model = \"ibm/granite-20b-code-instruct-v1-gptq\"\n",
    "# model = \"ibm-mistralai/mixtral-8x7b-instruct-v01-q\"\n",
    "# model = \"codellama/codellama-34b-instruct\"\n",
    "# model = \"codellama/codellama-70b-instruct\"\n",
    "# model = \"kaist-ai/prometheus-13b-v1\"\n",
    "# model = \"mistralai/mistral-7b-instruct-v0-2\"\n",
    "# model = \"thebloke/mixtral-8x7b-v0-1-gptq\"\n",
    "# llm = LangChainChatInterface(\n",
    "#    client=Client(credentials=Credentials.from_env()),\n",
    "#    model_id=model,\n",
    "#    parameters=TextGenerationParameters(\n",
    "#        decoding_method=DecodingMethod.SAMPLE,\n",
    "#        max_new_tokens=4096,\n",
    "#        min_new_tokens=10,\n",
    "#        temperature=0.1,\n",
    "#        top_k=50,\n",
    "#        top_p=1,\n",
    "#        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "#    ),\n",
    "#    moderations=ModerationParameters(\n",
    "#        # Threshold is set to very low level to flag everything (testing purposes)\n",
    "#        # or set to True to enable HAP with default settings\n",
    "#        hap=ModerationHAP(input=True, output=False, threshold=0.01)\n",
    "#    ),\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate.from_template(template_with_solved_example_and_analysis)\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=response_parser)\n",
    "\n",
    "model = model.replace(\"/\", \"_\")\n",
    "output_dir = \"output/\" + provider + \"_\" + model + \"/few_shot_pydantic/\"\n",
    "write_output_to_disk(\n",
    "    output_dir,\n",
    "    formatted_prompt,\n",
    "    src_file_contents,\n",
    "    src_file_from_quarkus_branch,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = chain.invoke(template_args)\n",
    "    with open(output_dir + \"result.txt\", \"a\") as f:\n",
    "        f.write(\"### Reasoning:\\n\")\n",
    "        for i in range(len(result[\"text\"].reasoning)):\n",
    "            f.write(result[\"text\"].reasoning[i])\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(result[\"text\"].updated_files)):\n",
    "            f.write(\"### Updated file \" + str((i + 1)) + \"\\n\")\n",
    "            f.write(result[\"text\"].updated_files[i].file_name + \":\")\n",
    "            f.write(result[\"text\"].updated_files[i].file_contents)\n",
    "    print(\"Saved result in \" + output_dir + \"result.txt\")\n",
    "except Exception as e:\n",
    "    with open(output_dir + \"traceback.txt\", \"a\") as f:\n",
    "        f.write(str(e))\n",
    "        f.write(traceback.format_exc())\n",
    "    print(\n",
    "        \"Something went wrong. A traceback was saved in \" + output_dir + \"traceback.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Reasoning:\\n\")\n",
    "for i in range(len(result[\"text\"].reasoning)):\n",
    "    print(result[\"text\"].reasoning[i])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(len(result[\"text\"].updated_files)):\n",
    "    print(\"### Updated file \" + str((i + 1)) + \"\\n\")\n",
    "    print(result[\"text\"].updated_files[i].file_name + \":\")\n",
    "    print(result[\"text\"].updated_files[i].file_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
