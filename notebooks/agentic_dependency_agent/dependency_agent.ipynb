{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Agent Testing\n",
    "\n",
    "This is an agent that will attempt to solve maven compiler errors using tools and an agentic approach\n",
    "\n",
    "We use LangGraph for implementing this.\n",
    "\n",
    "We create a network of agents with smart edges to determine what to do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Generation Agent\n",
    "\n",
    "We are using a few hardcoded examples of analysis issues to generate fixes.\n",
    "\n",
    "Before proceeding, make sure you are using Kai venv to run cells in this notebook.\n",
    "\n",
    "We need to install `langgraph` module, run following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kai.reactive_codeplanner.agentic.tools.tools import GetAllAvailableDependencies\n",
    "from pathlib import Path\n",
    "\n",
    "all_deps_tool = GetAllAvailableDependencies()\n",
    "\n",
    "print(all_deps_tool.get_input_schema().model_json_schema())\n",
    "print(all_deps_tool.tool_call_schema.model_json_schema())\n",
    "print(all_deps_tool.get_output_jsonschema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating tool usage with different providers via Model Provider\n",
    "\n",
    "This section validates that we are able to use tools with different Model Providers we support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState \n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from kai.llm_interfacing.model_provider import ModelProvider\n",
    "from kai.reactive_codeplanner.agentic.schemas.maven_compiler.schema import InitState\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def validate(model: ModelProvider, **kwargs):\n",
    "\n",
    "    def should_continue(state: InitState):\n",
    "        messages = state.messages\n",
    "        last_message = messages[-1]\n",
    "        print(f\"should conintue last message {last_message}\")\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            print(\"returning tools\") \n",
    "            return \"tools\"\n",
    "        return END\n",
    "\n",
    "\n",
    "    def call_model(state: InitState):\n",
    "        if state.dependencies: \n",
    "            print(f\"finished --> {state.dependencies}\")\n",
    "            return\n",
    "        messages = state.messages\n",
    "        response = model.bind_tools([GetAllAvailableDependencies()]).invoke(messages)\n",
    "        state.messages.append(response)\n",
    "        return state.model_dump()\n",
    "\n",
    "\n",
    "    graph_builder = StateGraph(InitState)\n",
    "    graph_builder.add_node(\"tools\", ToolNode([GetAllAvailableDependencies()]))\n",
    "    graph_builder.add_node(\"agent\", call_model)\n",
    "    graph_builder.add_edge(START, \"agent\")\n",
    "    graph_builder.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "    graph_builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    app = graph_builder.compile()\n",
    "\n",
    "    for r in app.stream({\"messages\": [HumanMessage(\"please list all the dependencies for the given java project\")], \"file_path\": \"/Users/shurley/repos/kai/kai/example/coolstore/pom.xml\", \"task\": {}, \"background\": \"\", \"dependencies\": []}, stream_mode=\"values\"):\n",
    "        print(r)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which model you test this with, you need to set up your API keys for the model before running the \"validation\" cell. Every cell has a comment at top about the environment variable expected.\n",
    "\n",
    "You will set the keys in `.env` file at the project root and run the following cell for changes to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|   Kai Model Provider   |      Model       |  Works?  |\n",
    "| ---------------------- | ---------------- | -------- |\n",
    "| ChatOpenAI             | gpt-3.5-turbo    |  &check; |\n",
    "| ChatOpenAI (RH Maas)   | granite-8b       |  &check; |\n",
    "| ChatOpenAI (RH Maas)   | llama-7b         |  &check; |\n",
    "| ChatBedrock            | llama-70b-v1     |  &cross; |\n",
    "| ChatBedrock            | llama-70b-v2     |  &cross; |\n",
    "| ChatBedrock            | claude-3-sonnet  |  &check; |\n",
    "| ChatGoogleGenAI        | gemini-2.0-flash |  &check; |\n",
    "| ChatDeepSeek           | deepseek-chat    |  &check; |\n",
    "\n",
    "Following model providers are not tested yet:\n",
    "\n",
    "* ChatOllama\n",
    "* ChatAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test with granite-8b on RH maas via ChatOpenAI\n",
    "## Env vars needed:\n",
    "## PARASOL_GRANITE_KEY - api key\n",
    "## PARASON_GRANITE_API - base url (usually ends with /v1)\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatOpenAI\n",
    "\n",
    "validate(ChatOpenAI(\n",
    "    api_key=os.environ[\"PARASOL_GRANITE_KEY\"],\n",
    "    base_url=os.environ[\"PARASOL_GRANITE_API\"],\n",
    "    model_name=\"granite-3-8b-instruct\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kai.llm_interfacing.model_provider import ChatOpenAI\n",
    "import os\n",
    "validate(ChatOpenAI(\n",
    "    api_key=os.environ[\"PARASOL_LLAMA_KEY\"],\n",
    "    base_url=os.environ[\"PARASOL_LLAMA_API\"],\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test with gpt-3.5-turbo via ChatOpenAI\n",
    "## Env vars needed:\n",
    "## OPENAI_API_KEY - api key\n",
    "\n",
    "validate(ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test with gemini-2.0-flash via GoogleGenAI\n",
    "## Env vars needed:\n",
    "## GEMINI_API_KEY - api key\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatGoogleGenerativeAI\n",
    "\n",
    "validate(ChatGoogleGenerativeAI(\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    model=\"gemini-2.0-flash\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test with llama-70b via Bedrock\n",
    "## Env vars needed:\n",
    "## AWS_ACCESS_KEY_ID\n",
    "## AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatBedrock\n",
    "\n",
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-2-1b-instruct-v1:0\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(ChatBedrock(\n",
    "    model=\"us.anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env vars needed:\n",
    "# DEEPSEEK_API_KEY\n",
    "\n",
    "from kai.llm_interfacing.model_provider import ChatDeepSeek\n",
    "\n",
    "validate(model=ChatDeepSeek(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "    model=\"deepseek-chat\",\n",
    "), interrupt_after=[\"tools\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tools with models that do not support tool calling\n",
    "\n",
    "In this section, we experiment creating our own _LangGraph ToolNode_ and custom logic in the _Conditional Edge_ to enable models that don't support tools to call given tools. Note that we saw examples of models in previous sections that do not support tools out of the box. It is absolutely essential that we are able to use tools with any model if we have to use LangGraph approach for agents. We validate whether its possible to do that in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use meta-llama-70b provided via Bedrock in this section\n",
    "\n",
    "import json\n",
    "from typing import Optional, TypedDict\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, SystemMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain.tools.render import render_text_description\n",
    "from kai.llm_interfacing.model_provider import ChatBedrock\n",
    "\n",
    "def _print_state(state: MessagesState):\n",
    "    print(\"...\"*15)\n",
    "    for idx, message in enumerate(state[\"messages\"]):\n",
    "        print(f\"{idx}: {message.content.splitlines()[0] if message.content else message.content}\")\n",
    "    print(\"...\"*15)\n",
    "\n",
    "@tool\n",
    "def get_temperature(city: Annotated[str, \"Name of the city\"]):\n",
    "    \"\"\"Returns current temperature in given city\"\"\"\n",
    "    return f\"Current temperature in {city} is 80 degrees.\"\n",
    "\n",
    "# define the tool node\n",
    "tool_node = ToolNode(tools=[get_temperature])\n",
    "\n",
    "# define the model provider\n",
    "model_provider = ChatBedrock(\n",
    "    model=\"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    ")\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extracts JSON code block from a multiline string.\"\"\"\n",
    "    pattern = r\"```(json)?\\s*(\\{.*?\\})\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(2)\n",
    "        try:\n",
    "            return json.loads(json_str)  # Convert to Python dict\n",
    "        except json.JSONDecodeError:\n",
    "            return None  # Invalid JSON format\n",
    "\n",
    "    return None\n",
    "\n",
    "# define the agent\n",
    "def agent_call(state: MessagesState):\n",
    "    # explain the tools here\n",
    "    sys_msg = f\"\"\"You are an intelligent assistant who can help users with their weather related queries. \n",
    "You may not know answers to all questions as some of the answers may depend on real time weather data. However, you are given tools you can use to access that data.\n",
    "Here is the schema of tools you are given:\n",
    "\n",
    "{render_text_description([get_temperature])}\n",
    "\n",
    "If you do need to call a tool, respond with a JSON object containing only two keys - tool_name and args. `tool_name` should be the name of the tool to call and `args` should be nested JSON containing the arguments to pass to the function in key value format.\n",
    "Make sure you always use ``` at the start and end of the JSON block to clearly separate it from text.\n",
    "If you have reached the answer, respond with text and add string 'FINAL ANSWER' to the beginning.\n",
    "\"\"\"\n",
    "    # insert tool description as sys message\n",
    "    if \"messages\" not in state or not state[\"messages\"] or \\\n",
    "        not isinstance(state[\"messages\"][0], SystemMessage):\n",
    "        state[\"messages\"].insert(0, SystemMessage(content=sys_msg))\n",
    "    \n",
    "    # ToolMessage is a special type of message that contains the tool response.\n",
    "    # our model doesn't understand that. we change it to a human message instead.\n",
    "    if isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        tool_output = state[\"messages\"][-1]\n",
    "        state[\"messages\"][-1] = HumanMessage(\n",
    "            content=f\"The output of tool {tool_output.name} is - {tool_output.content}\"\n",
    "        )\n",
    "    \n",
    "    response = model_provider.invoke(state[\"messages\"])\n",
    "\n",
    "    # our model does not set tool_calls on the response\n",
    "    # we parse the model response ourselves and convert\n",
    "    # it into an AIMessage with tool_calls set\n",
    "    if \"tool_name\" in response.content:\n",
    "        agent_response = extract_json_from_text(response.content)\n",
    "        if not agent_response:\n",
    "            return { \"messages\": [AIMessage(content=\"RETRY\")] }\n",
    "        response = AIMessage(\n",
    "            content=\"\",\n",
    "            tool_calls=[{\n",
    "                \"name\": agent_response.get(\"tool_name\"),\n",
    "                \"args\": agent_response.get(\"args\"),\n",
    "                \"type\": \"tool_call\",\n",
    "                \"id\": \"tool_call_id_1\",\n",
    "            }],\n",
    "        )\n",
    "    return { \"messages\": [response] }\n",
    "\n",
    "# this is the conditional edge to parse and\n",
    "# route the llm response to a tool\n",
    "def should_call_tool(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    if not last_message.content or \"RETRY\" in last_message.content:\n",
    "        return \"agent\"\n",
    "    return END\n",
    "    \n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent_call)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_call_tool, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in app.stream(\n",
    "    {\"messages\": [(\"human\", \"what's the weather in New York?\")]}, stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk[\"messages\"][-1].pretty_repr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
