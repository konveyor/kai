log_level = "info"
demo_mode = false
trace_enabled = true

[incident_store]
provider = "postgresql"

[incident_store.args]
host = "127.0.0.1"
database = "kai"
user = "kai"
password = "dog8code"

# **IBM served granite**
# ```
# [models]
#   provider = "ChatIBMGenAI"

#   [models.args]
#   model_id = "ibm/granite-13b-chat-v2"
# ```

# **IBM served mistral**
# ```
# [models]
#   provider = "ChatIBMGenAI"

#   [models.args]
#   model_id = "mistralai/mixtral-8x7b-instruct-v01"
# ```

# **IBM served codellama**
# ```
# [models]
#   provider = "ChatIBMGenAI"

#   [models.args]
#   model_id = "meta-llama/llama-2-13b-chat"
# ```

# **IBM served llama3**
# ```
#   # Note:  llama3 complains if we use more than 2048 tokens
#   # See:  https://github.com/konveyor-ecosystem/kai/issues/172
# [models]
#   provider = "ChatIBMGenAI"

#   [models.args]
#   model_id = "meta-llama/llama-3-70b-instruct"
#   parameters.max_new_tokens = 2048
# ```

# **Ollama**
# ```
# [models]
#   provider = "ChatOllama"

#   [models.args]
#   model = "mistral"
# ```

# **OpenAI GPT 4**
# ```
# [models]
#   provider = "ChatOpenAI"

#   [models.args]
#   model = "gpt-4"
# ```

# **OpenAI GPT 3.5**
# ```
# [models]
#   provider = "ChatOpenAI"

#   [models.args]
#   model = "gpt-3.5-turbo"
# ```

#[models]
#provider = "ChatIBMGenAI"

#[models.args]
#model_id = "mistralai/mixtral-8x7b-instruct-v01"

[embeddings]
todo = true

# **IBM served mistral**
[models]
provider = "ChatIBMGenAI"

[models.args]
model_id = "mistralai/mixtral-8x7b-instruct-v01"
